‚∏ª

LLM, Llama, Deepseek R1, LangChain, RAG, Vector DB, LLM Finetuning

This notebook has been created to demonstrate how to experiment with a Local Large Language Model (LLM) in a local environment.

‚∏ª

1. LLM (Large Language Model)

An LLM is a language model with an extremely large number of parameters, showing high performance in various areas of Natural Language Processing (NLP).
	‚Ä¢	Examples: GPT-4, Llama3, Phi4, Deepseek R1, etc.

‚∏ª

2.1 Llama

Llama is a large language model family released by Meta.
	‚Ä¢	Pretrained Llama models can be run locally via Hugging Face Transformers.

2.2 Deepseek R1

Deepseek R1 is an inference model based on reinforcement learning (RL).
	‚Ä¢	DeepSeek is a Chinese AI company established in 2023 by the High-Flyer hedge fund. They have released various open-source large language models such as DeepSeek LLM, DeepSeek Coder, DeepSeek Math, and so on.
In particular, the recently released DeepSeek-V3 has gained attention for performance on par with Claude 3.5 Sonnet or Gemini 1.5 Pro.
	‚Ä¢	Recently, they open-sourced DeepSeek-R1 and DeepSeek-R1-Zero, which maximize inference abilities through reinforcement learning (RL). These two models were open-sourced on January 20, 2025.

‚∏ª

3. LangChain

LangChain is a Python-based library that provides a variety of features to systematically develop and scale applications that utilize large language models (LLMs).
	‚Ä¢	Prompt Management
	‚Ä¢	Organizes and systematizes multiple prompts for efficient reuse and management.
	‚Ä¢	Chaining
	‚Ä¢	Allows you to sequentially connect multiple steps of LLM tasks in a pipeline format.
	‚Ä¢	Example: Summarization ‚Üí Q&A ‚Üí Sentiment Analysis, etc. You can easily implement multi-step workflows.
	‚Ä¢	Agents
	‚Ä¢	Modules that can dynamically perform tasks by accessing specific tools (API, DB, etc.) and make reasoning-based decisions to solve problems.
	‚Ä¢	Memory
	‚Ä¢	A feature that manages previous conversations or context so that the model can ‚Äúremember‚Äù them.
	‚Ä¢	Tool Integration
	‚Ä¢	Integrates easily with a wide range of external tools (databases, third-party APIs, web search, etc.) to provide rich functionality.

Because all of these capabilities can be combined, it is easy to rapidly prototype LLM-based applications and expand them into production environments.

‚∏ª

4. RAG (Retrieval-Augmented Generation)

RAG (Retrieval-Augmented Generation) is a method in which a large language model does not rely solely on its internally learned parameters but also queries external knowledge bases (documents, databases, web search results, etc.) in real time to retrieve and use necessary information.
	1.	Retrieve
	‚Ä¢	Uses a search engine such as a Vector DB to find relevant documents or information corresponding to the user query (or conversation context).
	‚Ä¢	Through similarity search based on embedding vectors, the model quickly obtains the information it needs.
	2.	Generate
	‚Ä¢	The LLM generates the answer using the retrieved documents.
	‚Ä¢	Leveraging specific details from the documents, the model returns factually rich and highly accurate text.

Advantages
	‚Ä¢	Improved Accuracy: Can utilize the latest data or knowledge not contained in the model‚Äôs training.
	‚Ä¢	Overcoming Memory Limits: You do not have to store all knowledge within model parameters, allowing you to maintain an efficient model size.
	‚Ä¢	Flexibility: Can combine various data types (text, image captions, DB content, etc.).

‚∏ª

5. Vector DB

A Vector DB is a specialized database for storing embedding vectors (from text, images, etc.) and performing fast searches for vectors (documents, images) that have high similarity.
	‚Ä¢	Key Features
	1.	Vector Insertion
	‚Ä¢	Converts text/images to vectors using a pretrained model (e.g., SentenceTransformer, BERT, etc.) and stores them.
	2.	Similarity Search (ANN Search)
	‚Ä¢	Utilizes Approximate Nearest Neighbor Search techniques to efficiently find the most similar vectors (documents) among a large vector set.
	3.	Scalability
	‚Ä¢	Maintains fast search speed even for very large datasets through scaling and distributed processing.
	‚Ä¢	Representative Vector DB examples
	‚Ä¢	FAISS: A vector similarity search library developed by Meta (formerly Facebook).
	‚Ä¢	Chroma: A vector DB that can easily scale from personal projects to enterprise services.
	‚Ä¢	Milvus: A high-performance, large-scale vector search engine.
	‚Ä¢	Pinecone: A fully managed cloud-based vector DB service.

Use Cases
	‚Ä¢	RAG (Retrieval-Augmented Generation) for document search
	‚Ä¢	Similarity-based recommendation systems
	‚Ä¢	Multi-modal search for images, audio, etc.

‚∏ª

6. LLM Finetuning

LLM Finetuning is the process of additional training a pretrained large language model (e.g., GPT, BERT, etc.) to customize it for a specific task or domain.
	‚Ä¢	You can significantly improve the model‚Äôs performance on specific data while reusing existing parameters.

6.1 Finetuning Methods
	1.	Full Finetuning
	‚Ä¢	Updating all model parameters, usually through epoch-based optimization.
	‚Ä¢	Used when you have plenty of data and computational resources.
	2.	PEFT (LoRA, Prefix Tuning, etc.)
	‚Ä¢	Approaches that update only part of the parameters or that enable low-cost additional training.
	‚Ä¢	LoRA (Low-Rank Adaptation): Trains certain weight matrices in the model in a low-rank form to achieve finetuning effects with less memory usage.
	‚Ä¢	Prefix Tuning: Adds a virtual prompt (prefix) before the input tokens, guiding model performance improvement without major changes to the main model.
	3.	Training Tools
	‚Ä¢	Hugging Face Transformers library‚Äôs Trainer API
	‚Ä¢	Deepspeed, Accelerate for distributed training and memory optimization
	‚Ä¢	PEFT library: Apply LoRA and various other techniques easily

6.2 Considerations
	‚Ä¢	Data Quality: The domain suitability and label quality of the finetuning data are very important.
	‚Ä¢	Avoid Overfitting: Simply increasing the learning rate or the number of epochs may cause the model‚Äôs generative ability to degrade or lead to model bias.
	‚Ä¢	Model Compatibility: Some models have architectural constraints that limit finetuning, so you should consult official documentation or community resources.

‚∏ª

By combining LangChain, RAG, Vector DB, and LLM Finetuning, you can:
	1.	Easily build an LLM-based pipeline,
	2.	Strengthen the model‚Äôs responses with accurate and rich knowledge, and
	3.	Create a custom LLM optimized for a specific domain or work environment.

In practical or research settings where LLMs are utilized:
	‚Ä¢	Use LangChain to structure your workflow,
	‚Ä¢	Enhance it with RAG for real-time knowledge retrieval,
	‚Ä¢	Maximize search performance with a Vector DB,
	‚Ä¢	And LLM Finetuning to adapt the model to domain-specific needs.

This will enable you to build more efficient and powerful NLP solutions.

‚∏ª

Notebook Demonstration

In this notebook, we will explore the following simple example code to see how to use a local environment:
	1.	Load a local Llama model (or similar) and perform simple inference
	2.	Use LangChain + Vector DB to demonstrate RAG
	3.	(Simple version) LLM Finetuning example

pip install -r requirements.txt

Run this in your terminal to set up the environment.

Example: Loading a local LLM
Below is an example of downloading the Llama3.1-8b model from the Hugging Face model repository to your local machine and performing a simple inference.

On Apple Silicon (M1, M4, etc.), mps (Metal Performance Shaders) device support may be set automatically or require manual configuration.

‚∏ª

Tokens and Tokenizer

In Natural Language Processing (NLP), tokenization is a very important preprocessing step that breaks text into specific units. The tool or library used in this process is the tokenizer. Let‚Äôs learn about this in more detail.
	1.	What is a Token?

A token is a small semantic unit obtained by splitting text (sentences, paragraphs, etc.). The way you split can vary, producing different types of token units.

1.1 Word-level
	‚Ä¢	A simple method that splits based on whitespace or punctuation.
	‚Ä¢	Example: ‚ÄúÎÇòÎäî ÌïôÍµêÏóê Í∞ÑÎã§.‚Äù ‚Üí [‚ÄúÎÇòÎäî‚Äù, ‚ÄúÌïôÍµêÏóê‚Äù, ‚ÄúÍ∞ÑÎã§.‚Äù]

1.2 Morpheme-level (used in Korean)
	‚Ä¢	Often used for Korean.
	‚Ä¢	Example: ‚ÄúÎÇòÎäî ÌïôÍµêÏóê Í∞ÑÎã§.‚Äù ‚Üí [‚ÄúÎÇò‚Äù, ‚ÄúÎäî‚Äù, ‚ÄúÌïôÍµê‚Äù, ‚ÄúÏóê‚Äù, ‚ÄúÍ∞Ä‚Äù, ‚Äú·ÑÇÎã§‚Äù, ‚Äú.‚Äù]

1.3 Subword-level
	‚Ä¢	Uses algorithms like BPE, WordPiece, SentencePiece to split into smaller-than-word units.
	‚Ä¢	Example: ‚Äúunhappy‚Äù ‚Üí [‚Äúun‚Äù, ‚Äúhappy‚Äù]
	‚Ä¢	Example: ‚Äúunbelievable‚Äù ‚Üí [‚Äúun‚Äù, ‚Äúbelievable‚Äù]

	2.	What is a Tokenizer?

A tokenizer is a tool (or library) that splits text into tokens according to certain rules or algorithms.

2.1 Rule-based Tokenizer
	‚Ä¢	Splits using predefined rules such as whitespace, punctuation, or patterns (regex).
	‚Ä¢	Example: split(), simple regex-based splitting.

2.2 Trained Tokenizer
	‚Ä¢	Automatically creates a token vocabulary by training on a corpus of data, learning the rules.
	‚Ä¢	BPE (Byte-Pair Encoding), WordPiece, SentencePiece are representative examples.
	‚Ä¢	Also used in large language models (e.g., BERT, GPT).

	3.	Why is Tokenization Important?
	4.	Improved Accuracy
	‚Ä¢	Splitting text into correct units increases the accuracy of subsequent tasks like morphological analysis or POS tagging.
	5.	Vocabulary Management
	‚Ä¢	If you only split by words, your vocabulary can become huge, but with subword methods, you can handle rare words and neologisms efficiently.
	6.	Maximize Model Performance
	‚Ä¢	Poor tokenization can make it difficult to train the model or degrade performance at inference time.
	‚Ä¢	Models like BERT or GPT are trained on consistent tokenization rules.
	7.	Considerations for Korean Tokenization
	8.	The need for morphological analysis
	‚Ä¢	Korean has various postpositions and endings, making it difficult to tokenize by whitespace alone.
	9.	Various elements within a word
	‚Ä¢	‚ÄúÌïôÍµêÏóê‚Äù ‚Üí [‚ÄúÌïôÍµê‚Äù, ‚ÄúÏóê‚Äù]
	‚Ä¢	‚ÄúÌïôÍµêÏóêÏÑú‚Äù ‚Üí [‚ÄúÌïôÍµê‚Äù, ‚ÄúÏóêÏÑú‚Äù]
	‚Ä¢	Must split postpositions, endings, etc. to get the desired accuracy.
	10.	Irregular spacing
	‚Ä¢	Because spacing is often not strictly followed, purely rule-based approaches have limitations.
	11.	Representative Tokenizers

	‚Ä¢	NLTK (English)
	‚Ä¢	One of the most widely used NLP libraries in Python
	‚Ä¢	Provides word tokenization, sentence tokenization, stopword removal, etc.
	‚Ä¢	KoNLPy (Korean)
	‚Ä¢	Integrates multiple morphological analyzers (Twitter, Kkma, Hannanum, etc.)
	‚Ä¢	Specialized for Korean, offering morphological analysis, POS tagging, etc.
	‚Ä¢	BPE, SentencePiece, WordPiece
	‚Ä¢	Subword-based tokenizers
	‚Ä¢	Used in large language models such as BERT, GPT, RoBERTa, etc.

	6.	Summary

	‚Ä¢	Token
	‚Ä¢	The result of splitting text into small units (words, morphemes, subwords, etc.)
	‚Ä¢	Tokenizer
	‚Ä¢	A tool or algorithm for splitting text according to a specified criterion
	‚Ä¢	Tokenization
	‚Ä¢	A core preprocessing step in NLP. Good tokenization can significantly improve model performance and manage vocabulary effectively.
	‚Ä¢	For Korean, morphological analysis or subword-based methods are commonly used.

In conclusion, proper tokenization can greatly improve data quality and model performance. It is important to choose a tokenizer suited to your NLP project and devise a tokenization strategy that reflects the linguistic characteristics of each language.

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# Example for a Llama model (In practice, you may need permission or a different model name in Hugging Face.)
# model_name = "meta-llama/Llama-3.1-8B"
model_name="../models/Llama-3.1-8b"  
# Check if the mps device is available
device = torch.device("mps") if torch.backends.mps.is_available() else torch.device("cpu")
print("Using device:", device)

# Load tokenizer and model
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map={"": device})

# Simple text generation example
prompt = "Hello, how are you?"
inputs = tokenizer(prompt, return_tensors="pt").to(device)
with torch.no_grad():
    outputs = model.generate(**inputs, max_new_tokens=50)
generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
print("Generated text:")
print(generated_text)

Using device: mps
...
Generated text:
Hello, how are you? Today we are going to talk about the new generation...

Explanation:
	‚Ä¢	torch_dtype=torch.float16: float16 is optimal on Apple MPS.
	‚Ä¢	device_map={"": device}: Automatically places the model on MPS (GPU) or CPU.
	‚Ä¢	max_new_tokens=50: Limits response to 50 tokens.

Llama 3.1 8B + LangChain + Vector DB (RAG)

You can also integrate the Llama 3.1 8B model with LangChain and ChromaDB (a Vector DB) to perform Retrieval-Augmented Generation (RAG).

2.1 Install LangChain & ChromaDB

pip install langchain chromadb faiss-cpu sentence-transformers

2.2 RAG Code Example

# Required library imports
from transformers import pipeline  # <-- added here
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.docstore.document import Document
from langchain.chains import RetrievalQA
from langchain.llms import HuggingFacePipeline

# 1) Load embedding model
embedding_model = "sentence-transformers/all-MiniLM-L6-v2"
embeddings = HuggingFaceEmbeddings(model_name=embedding_model)

# Sample documents
texts = [
    "LangChain is a framework for chaining LLMs.",
    "RAG stands for Retrieval-Augmented Generation.",
    "A Vector DB is a database for searching document embedding vectors."
]
documents = [Document(page_content=t) for t in texts]

# 2) Initialize a Chroma VectorStore
vectorstore = Chroma.from_documents(documents, embedding=embeddings, collection_name="example_collection")

# 3) Connect Llama 3.1 8B model to LangChain
generator_pipeline = pipeline(
    "text-generation",
    model=model,  # Llama 3.1 8B model
    tokenizer=tokenizer
)
llm = HuggingFacePipeline(pipeline=generator_pipeline)

# 4) Create a RetrievalQA chain
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=vectorstore.as_retriever()
)

# 5) Ask a question and generate RAG-based answer
query = "What is RAG?"
answer = qa_chain.run(query)
print(f"Q: {query}\nA: {answer}")

Output:

Q: What is RAG?
A: Retrieval-Augmented Generation...



‚∏ª

What is an Embedding Model?

1) Concept

An Embedding Model is a model that converts data into fixed-size vectors.
It transforms various types of data (text, images, audio, etc.) into numerical vectors so that computers can understand and process them.

In other words, embedding is the process of converting high-dimensional data (words, sentences, documents, images, etc.) into dense vectors, and the model that generates these vectors is called an Embedding Model.

‚∏ª

2) Why are embeddings needed?

Because computers can only understand numbers, they cannot directly process natural language (NLP) or images.
Hence, Embedding Models are needed for the following reasons:
	‚Ä¢	Convert strings to numbers: Text must be converted into numerical vectors for machine learning models to understand.
	‚Ä¢	Similar data have similar vector values: Words with similar meanings end up with similar vector representations.
	‚Ä¢	Compress high-dimensional data into lower dimensions: Embeddings help reduce the dimensionality for more optimized computations.

For example, ‚Äúcat‚Äù and ‚Äúdog‚Äù are similar in meaning, so their embedding vectors are close to each other in the vector space.
Meanwhile, ‚Äúcat‚Äù and ‚Äúcar‚Äù have little connection, so they lie far apart in the vector space.

‚∏ª

3) Types of Embedding Models

Embedding Models are used to vectorize text, images, audio, etc. Here are some representative types:

(1) Text Embeddings (Word/Sentence Embeddings)
	‚Ä¢	Models that convert words, sentences, or documents into vectors for NLP
	‚Ä¢	Uses: Chatbots, search, recommendation systems, document classification, RAG

Representative Models:
	‚Ä¢	Word2Vec: A leading model for word vectorization
	‚Ä¢	GloVe: Vectors that reflect statistical co-occurrence of words
	‚Ä¢	FastText: An improvement on Word2Vec (can embed subwords)
	‚Ä¢	BERT Embedding: A powerful model that considers context when creating vectors
	‚Ä¢	Sentence-BERT (SBERT): A model that creates embeddings at the sentence level
	‚Ä¢	sentence-transformers/all-MiniLM-L6-v2: A lightweight, fast sentence embedding model (often used with LangChain, RAG)

Example (using sentence-transformers to generate text embedding vectors):

from sentence_transformers import SentenceTransformer

# Load an SBERT-based sentence embedding model
model = SentenceTransformer("all-MiniLM-L6-v2")

# Example sentences
sentences = ["Cats are cute.", "Dogs are loyal.", "Cars are fast."]

# Convert to embeddings (vectors)
embeddings = model.encode(sentences)

# Print the vector for the first sentence
print(embeddings[0])



‚∏ª

(2) Image Embeddings
	‚Ä¢	Convert image data into vectors for tasks like image similarity search, object recognition, etc.
	‚Ä¢	Uses: Image retrieval systems, style recommendations, computer vision

Representative Models:
	‚Ä¢	ResNet-50, EfficientNet, CLIP: For image classification and feature extraction
	‚Ä¢	DINOv2: An image embedding model recently introduced by Meta
	‚Ä¢	OpenAI CLIP: Maps text and images into the same vector space (e.g. ‚Äúdog photo‚Äù ‚Üí close to dog-image vectors)

Example (using CLIP to generate image embedding vectors):

import torch
import clip
from PIL import Image

# Load CLIP model
device = "cuda" if torch.cuda.is_available() else "cpu"
model, preprocess = clip.load("ViT-B/32", device=device)

# Load and transform the image
image = preprocess(Image.open("dog.jpg")).unsqueeze(0).to(device)

# Extract embedding vectors
with torch.no_grad():
    image_features = model.encode_image(image)

print(image_features.shape)  # e.g., [1, 512]



‚∏ª

(3) Audio Embeddings
	‚Ä¢	Convert audio data into vectors for tasks like speech recognition, emotion analysis, etc.
	‚Ä¢	Uses: Speech-to-text, emotion analysis, noise filtering, music recommendation systems

Representative Models:
	‚Ä¢	MFCC (Mel-Frequency Cepstral Coefficients): A method to extract feature vectors from audio signals
	‚Ä¢	wav2vec 2.0 (by Facebook): Converts audio to vectors and can transcribe text
	‚Ä¢	Whisper (by OpenAI): A multi-language speech recognition and embedding model

Example (using librosa to generate audio embedding vectors):

import librosa

# Load an audio file
y, sr = librosa.load("speech.wav")

# Extract MFCC feature vectors
mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)

# Print the MFCC vector for the first frame
print(mfccs[:, 0])



‚∏ª

4) Applications of Embedding Vectors

Using embedding models, you can achieve strong performance in tasks such as document search, chatbots, recommendation systems, and RAG models.
	‚Ä¢	Document Retrieval System
	‚Ä¢	Convert user queries into vectors and search for the most similar documents.
	‚Ä¢	Example: LangChain + ChromaDB for RAG (Retrieval-Augmented Generation)
	‚Ä¢	Chatbots and LLM Applications
	‚Ä¢	During chatbot response generation, utilize embedding-based searches.
	‚Ä¢	Example: Convert user input into embeddings and retrieve the most relevant answers from a DB.
	‚Ä¢	Recommendation Systems
	‚Ä¢	Platforms like Netflix or Spotify use embedding vectors for user preference and content.
	‚Ä¢	Example: Embedding vectors for movies/music the user has consumed, then recommend similar content.
	‚Ä¢	Healthcare and Bioinformatics
	‚Ä¢	Vectorize gene data, medical publications, protein structures, etc. for analysis.
	‚Ä¢	Example: Drug discovery, genome analysis

‚∏ª

5) Conclusion

Embedding models convert text, images, audio into vector representations, enabling similarity search, chatbots, recommendation systems, and more.
Transformer-based embedding models (e.g., BERT, CLIP, wav2vec 2.0, etc.) are becoming more sophisticated, enabling richer semantic representations.

üöÄ By combining LangChain + Vector DB (Chroma, FAISS) + LLM (RAG), you can build even more powerful AI applications!

‚∏ª

3) Llama 3.1 8B Model Finetuning (LoRA Application)

To finetune the Llama 3.1 8B model using LoRA (Low-Rank Adaptation), you can use PEFT (Parameter-Efficient Fine-Tuning).

3.1 Install LoRA

pip install peft bitsandbytes datasets

3.2 LoRA Code Example

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer
from peft import LoraConfig, get_peft_model
from datasets import Dataset

# Load model and tokenizer
model_name = "../models/Llama-3.1-8b"
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
print(f"Using device: {device}")

# Load the model
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,  # ‚úÖ using bfloat16 instead of float16
    device_map="auto"
).to(device)  # explicitly move model to device

# LoRA Configuration
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.1,
    bias="none",
    target_modules=["q_proj", "v_proj"]  # Adjust core weights of Llama
)

# Apply LoRA to the model
lora_model = get_peft_model(model, lora_config)

# ‚úÖ Prepare input data
train_texts = [
    "Question: What is the Llama model?\nAnswer: It is a large language model by Meta.",
    "Question: What is RAG?\nAnswer: It is a retrieval-based generation model."
]

# ‚úÖ Modified tokenization function
def tokenize_fn(text):
    tokenized = tokenizer(text, truncation=True, padding="max_length", max_length=64)
    return {
        "input_ids": tokenized["input_ids"],
        "attention_mask": tokenized["attention_mask"],
        "labels": tokenized["input_ids"]  # GPT-style models use labels = input_ids
    }

# ‚úÖ Convert to Hugging Face Dataset
train_dataset = Dataset.from_dict({"text": train_texts})
train_dataset = train_dataset.map(lambda x: tokenize_fn(x["text"]), batched=True, remove_columns=["text"])

# ‚úÖ TrainingArguments (disable removal of unused columns)
training_args = TrainingArguments(
    output_dir="finetuned-llama3",
    per_device_train_batch_size=1,
    num_train_epochs=1,
    save_steps=10,
    logging_steps=5,
    remove_unused_columns=False,  # <--- Important
)

# ‚úÖ Trainer setup
trainer = Trainer(
    model=lora_model,
    args=training_args,
    train_dataset=train_dataset,
)

# ‚úÖ Start training
trainer.train()

# ‚úÖ Save the trained model
trainer.save_model("finetuned-llama3")

# Save the LoRA adapter checkpoint
lora_model.save_pretrained("finetuned-llama3")
# Optionally save the tokenizer too
tokenizer.save_pretrained("finetuned-llama3")
print("Adapter and tokenizer saved successfully!")

print("Finetuning Complete!")

Using device: mps
...
Finetuning Complete!

Why LoRA?
	‚Ä¢	Typical training for Llama 3.1 8B might require over 100GB of RAM.
	‚Ä¢	By adjusting only certain weights (Q, V projections) using LoRA, you can reduce memory usage.
	‚Ä¢	Can run even on an M4 Pro with 64GB RAM.

4) Summary
	‚Ä¢	Llama 3.1 8B model usage
	1.	Use Hugging Face Transformers
	‚Ä¢	Use mps (Apple Metal)
	‚Ä¢	Use float16 for memory optimization
	‚Ä¢	Integrate with LangChain + Vector DB (RAG)
	2.	Use ChromaDB to store documents
	‚Ä¢	Build retrieval-answer systems with LangChain
	‚Ä¢	Finetune Llama 3.1 8B (LoRA)
	3.	Apply PEFT (LoRA) for lightweight training
	‚Ä¢	Possible on Apple Silicon

How to Use the Trained Model

from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

# Load the base model from its original checkpoint
base_model_name = "../models/Llama-3.1-8b"
base_model = AutoModelForCausalLM.from_pretrained(base_model_name)

# Now load the adapter from your local directory
model = PeftModel.from_pretrained(base_model, "finetuned-llama3", local_files_only=True)

# Load the tokenizer (either from your adapter folder or the base model)
tokenizer = AutoTokenizer.from_pretrained("finetuned-llama3")

prompt = "Question: What is the Llama model?\nAnswer:"
inputs = tokenizer(prompt, return_tensors="pt")

with torch.no_grad():
    outputs = model.generate(**inputs, max_new_tokens=50)

generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
print("Generated Response:")
print(generated_text)

Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Generated Response:
Question: What is the Llama model?
Answer: B

It seems the model did not learn well‚Äîlikely because the training dataset was very small. If you increase the dataset size, results will likely improve.

For example, you could try a Korean food dataset from:
https://huggingface.co/datasets/SGTCho/korean_food

And follow the steps here:
https://github.com/SGT-Cho/LLM/tree/main/Finetuning

‚∏ª


# LLM, Llama, Deepseek R1, LangChain, RAG, Vector DB, LLM Finetuning

Î≥∏ ÎÖ∏Ìä∏Î∂ÅÏùÄ **Î°úÏª¨ ÌôòÍ≤ΩÏóêÏÑú LLM(Local Large Language Model)ÏùÑ ÌôúÏö©ÌïòÏó¨ Ïã§Ìóò**ÌïòÎäî Î∞©Î≤ïÏùÑ Îç∞Î™®ÌïòÍ∏∞ ÏúÑÌï¥ ÏûëÏÑ±ÎêòÏóàÏäµÎãàÎã§.  

---

## 1. LLM (Large Language Model)

**LLM**ÏùÄ Î∞©ÎåÄÌïú ÌååÎùºÎØ∏ÌÑ∞Î•º Í∞ñÏ∂ò Ïñ∏Ïñ¥ Î™®Îç∏Î°ú, **ÏûêÏó∞Ïñ¥ Ï≤òÎ¶¨(NLP)** Ï†ÑÎ∞òÏóê Í±∏Ï≥ê **ÎÜíÏùÄ ÏÑ±Îä•**ÏùÑ Î≥¥ÏûÖÎãàÎã§.  
- Ïòà: **GPT-4**, **Llama3**, **Phi4**, **Deepseek R1** Îì±

---

## 2.1 Llama

**Llama**Îäî MetaÏóêÏÑú Í≥µÍ∞úÌïú **ÎåÄÍ∑úÎ™® Ïñ∏Ïñ¥ Î™®Îç∏** Í≥ÑÏó¥ÏûÖÎãàÎã§.  
- ÏÇ¨Ï†ÑÏóê ÌïôÏäµÎêú **Llama Î™®Îç∏**ÏùÑ Hugging Face TransformersÎ•º ÌÜµÌï¥ **Î°úÏª¨ÏóêÏÑú Ïã§Ìñâ** Í∞ÄÎä•Ìï©ÎãàÎã§.
  
## 2.2 Deepseek R1  
  
**Deepseek R1**ÏùÄ Í∞ïÌôî ÌïôÏäµ Í∏∞Î∞ò Ï∂îÎ°† Î™®Îç∏ÏûÖÎãàÎã§.
- DeepSeek Îäî Ï§ëÍµ≠ Ìó§ÏßÄ ÌéÄÎìúÏù∏ ÌïòÏù¥-ÌîåÎùºÏù¥Ïñ¥ (High-Flyer) Í∞Ä 2023ÎÖÑÏóê ÏÑ§Î¶ΩÌïú Ï§ëÍµ≠Ïùò Ïù∏Í≥µÏßÄÎä• ÌöåÏÇ¨ÏûÖÎãàÎã§. DeepSeek LLM, DeepSeek Coder, DeepSeek Math Îì± Îã§ÏñëÌïú ÎåÄÌòï Ïñ∏Ïñ¥ Î™®Îç∏ÏùÑ Ïò§ÌîàÏÜåÏä§Î°ú Í≥µÍ∞úÌï¥ ÏôîÏäµÎãàÎã§.  
ÌäπÌûà, ÏµúÍ∑º Í≥µÍ∞úÎêú DeepSeek-V3Îäî Claude 3.5 Sonnet Î∞è Gemini 1.5 ProÏôÄ Í≤¨Ï§Ñ ÎßåÌïú ÏÑ±Îä•ÏúºÎ°ú Ï£ºÎ™©Î∞õÍ≥† ÏûàÏäµÎãàÎã§.  

- Ïù¥Î≤àÏóêÎäî Í∞ïÌôî ÌïôÏäµ (RL, Reinforcement Learning) ÏùÑ ÌÜµÌï¥ Ï∂îÎ°† Îä•Î†•ÏùÑ Í∑πÎåÄÌôîÌïú ÏÉàÎ°úÏö¥ Î™®Îç∏, DeepSeek-R1 Í≥º DeepSeek-R1-Zero Î•º Í≥µÍ∞úÌïòÏòÄÏäµÎãàÎã§. Ïù¥ Îëê Î™®Îç∏ÏùÄ 2025ÎÖÑ 1Ïõî 20ÏùºÏóê Ïò§ÌîàÏÜåÏä§Î°ú Í≥µÍ∞úÎêòÏóàÏäµÎãàÎã§.
---

## 3. LangChain

**LangChain**ÏùÄ ÌååÏù¥Ïç¨ Í∏∞Î∞ò ÎùºÏù¥Î∏åÎü¨Î¶¨Î°ú, **ÎåÄÍ∑úÎ™® Ïñ∏Ïñ¥ Î™®Îç∏(LLM)ÏùÑ ÌôúÏö©Ìïú Ïï†ÌîåÎ¶¨ÏºÄÏù¥ÏÖò**ÏùÑ **Ï≤¥Í≥ÑÏ†ÅÏúºÎ°ú Í∞úÎ∞ú**ÌïòÍ≥† **ÌôïÏû•**ÌïòÍ∏∞ ÏúÑÌïú Îã§ÏñëÌïú Í∏∞Îä•ÏùÑ Ï†úÍ≥µÌï©ÎãàÎã§.

- **Prompt Í¥ÄÎ¶¨**  
  - Ïó¨Îü¨ ÌîÑÎ°¨ÌîÑÌä∏Î•º Ìö®Ïú®Ï†ÅÏúºÎ°ú Í¥ÄÎ¶¨ÌïòÍ≥† Ïû¨ÏÇ¨Ïö©Ìï† Ïàò ÏûàÎèÑÎ°ù **Ï≤¥Í≥ÑÌôî**Ìï©ÎãàÎã§.

- **Ï≤¥Ïù¥Îãù(Chaining)**  
  - Ïó¨Îü¨ Ïä§ÌÖùÏùò LLM ÏûëÏóÖÏùÑ **ÏàúÏ∞®Ï†ÅÏúºÎ°ú Ïó∞Í≤∞**Ìï¥ **ÌååÏù¥ÌîÑÎùºÏù∏** ÌòïÌÉúÎ°ú Íµ¨ÏÑ±Ìï† Ïàò ÏûàÏäµÎãàÎã§.  
  - Ïòà) ÏöîÏïΩ ‚Üí ÏßàÏùòÏùëÎãµ ‚Üí Í∞êÏÑ± Î∂ÑÏÑù Îì±, Îã§Îã®Í≥Ñ ÏõåÌÅ¨ÌîåÎ°úÏö∞Î•º Í∞ÑÎã®Ìûà Íµ¨ÌòÑ

- **ÏóêÏù¥Ï†ÑÌä∏(Agent)**  
  - ÌäπÏ†ï Ìà¥(API, DB Îì±)Ïóê Ï†ëÍ∑ºÌï¥ **ÎèôÏ†ÅÏúºÎ°ú ÏûëÏóÖ**ÏùÑ ÏàòÌñâÌïòÍ≥†, **Ï∂îÎ°†(Reasoning)** Í∏∞Î∞ò ÏùòÏÇ¨Í≤∞Ï†ïÏùÑ ÎÇ¥Î†§ **Î¨∏Ï†ú Ìï¥Í≤∞**ÏùÑ ÎèïÎäî Î™®Îìà

- **Î©îÎ™®Î¶¨(Memory)**  
  - Î™®Îç∏Ïù¥ **Ïù¥Ï†Ñ ÎåÄÌôîÎÇò Ïª®ÌÖçÏä§Ìä∏**Î•º **Í∏∞Ïñµ**Ìï† Ïàò ÏûàÎèÑÎ°ù Í¥ÄÎ¶¨ÌïòÎäî Í∏∞Îä•

- **Ìà¥ Ïó∞Îèô**  
  - Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§, ÏÑúÎìúÌååÌã∞ API, Ïõπ Í≤ÄÏÉâ Îì± Îã§ÏñëÌïú **Ïô∏Î∂Ä Ìà¥**Í≥º ÏÜêÏâΩÍ≤å Ïó∞ÎèôÌï¥ **ÌíçÎ∂ÄÌïú Í∏∞Îä•**ÏùÑ Ï†úÍ≥µ

Ïù¥ Î™®Îì† Í∏∞Îä•ÏùÑ **ÌÜµÌï©Ï†ÅÏúºÎ°ú** ÏÇ¨Ïö©Ìï† Ïàò ÏûàÏñ¥, **LLMÏùÑ ÌôúÏö©Ìïú Ïï†ÌîåÎ¶¨ÏºÄÏù¥ÏÖò**ÏùÑ Îπ†Î•¥Í≤å **ÌîÑÎ°úÌÜ†ÌÉÄÏù¥Ìïë**ÌïòÍ≥†, **ÏÉùÏÇ∞ ÌôòÍ≤Ω**ÏúºÎ°ú ÌôïÏû•ÌïòÍ∏∞ ÏâΩÏäµÎãàÎã§.

---

## 4. RAG (Retrieval-Augmented Generation)

**RAG(Retrieval-Augmented Generation)**Îäî ÎåÄÍ∑úÎ™® Ïñ∏Ïñ¥ Î™®Îç∏Ïù¥ Í∏∞Ï°¥Ïóê ÌïôÏäµÎêú **ÌååÎùºÎØ∏ÌÑ∞**ÏóêÎßå ÏùòÏ°¥ÌïòÏßÄ ÏïäÍ≥†, Î™®Îç∏ **Ïô∏Î∂Ä**Ïùò ÏßÄÏãù Î≤†Ïù¥Ïä§(Î¨∏ÏÑú, Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§, Ïõπ Í≤ÄÏÉâ Í≤∞Í≥º Îì±)ÏóêÏÑú **Ïã§ÏãúÍ∞Ñ**ÏúºÎ°ú Ï†ïÎ≥¥Î•º Í≤ÄÏÉâÌï¥ ÌôúÏö©ÌïòÎäî Î∞©ÏãùÏûÖÎãàÎã§.

1. **Retrieve (Í≤ÄÏÉâ)**  
   - ÏÇ¨Ïö©ÏûêÏùò ÏßàÏùò(ÎòêÎäî ÎåÄÌôî Îß•ÎùΩ)Ïóê ÎåÄÏùëÎêòÎäî **Í¥ÄÎ†® Î¨∏ÏÑúÎÇò Ï†ïÎ≥¥**Î•º Ï∞æÍ∏∞ ÏúÑÌï¥, **Vector DB** Îì±Ïùò Í≤ÄÏÉâ ÏóîÏßÑÏùÑ ÏÇ¨Ïö©  
   - **ÏûÑÎ≤†Îî© Î≤°ÌÑ∞** Í∏∞Î∞òÏùò Ïú†ÏÇ¨ÎèÑ Í≤ÄÏÉâÏùÑ ÌÜµÌï¥, Î™®Îç∏Ïù¥ ÌïÑÏöîÌïú Ï†ïÎ≥¥Î•º **Îπ†Î•¥Í≤å ÌöçÎìù**

2. **Generate (ÏÉùÏÑ±)**  
   - Í≤ÄÏÉâÎêú Î¨∏ÏÑúÎ•º ÌÜ†ÎåÄÎ°ú **LLM**Ïù¥ **ÏùëÎãµ**ÏùÑ ÏÉùÏÑ±  
   - Î™®Îç∏ÏùÄ **Î¨∏ÏÑúÏùò Íµ¨Ï≤¥Ï†ÅÏù∏ ÎÇ¥Ïö©**ÏùÑ Î∞îÌÉïÏúºÎ°ú, **ÏÇ¨Ïã§Ï†ÅÏúºÎ°ú ÌíçÎ∂Ä**ÌïòÍ≥† **Ï†ïÌôïÎèÑ ÎÜíÏùÄ** ÌÖçÏä§Ìä∏Î•º Î∞òÌôò

**Ïû•Ï†ê**  
- **Ï†ïÌôïÎèÑ Ìñ•ÏÉÅ**: ÏµúÏã† Ï†ïÎ≥¥ÎÇò Î™®Îç∏Ïù¥ ÌïôÏäµÌïòÏßÄ Î™ªÌïú ÏßÄÏãùÏùÑ ÌôúÏö© Í∞ÄÎä•  
- **Î©îÎ™®Î¶¨ Ï†úÌïú Í∑πÎ≥µ**: Î™®Îì† ÏßÄÏãùÏùÑ Î™®Îç∏ ÌååÎùºÎØ∏ÌÑ∞Ïóê Îã¥ÏßÄ ÏïäÏïÑÎèÑ ÎêòÎØÄÎ°ú, Î™®Îç∏ ÌÅ¨Í∏∞Î•º **Ìö®Ïú®Ï†ÅÏúºÎ°ú Ïú†ÏßÄ**Ìï† Ïàò ÏûàÏùå  
- **Ïú†Ïó∞ÏÑ±**: Îã§ÏñëÌïú ÌòïÌÉúÏùò Îç∞Ïù¥ÌÑ∞(ÌÖçÏä§Ìä∏, Ïù¥ÎØ∏ÏßÄ ÏÑ§Î™Ö, DB ÎÇ¥Ïö© Îì±)ÏôÄ Í≤∞Ìï© Í∞ÄÎä•

---

## 5. Vector DB

**Vector DB**Îäî ÌÖçÏä§Ìä∏ÎÇò Ïù¥ÎØ∏ÏßÄÎ•º **ÏûÑÎ≤†Îî© Î≤°ÌÑ∞**Î°ú Î≥ÄÌôòÌïòÏó¨ Ï†ÄÏû•ÌïòÍ≥†, Ïù¥ÏôÄ **Ïú†ÏÇ¨ÎèÑÍ∞Ä ÎÜíÏùÄ** Î≤°ÌÑ∞(Î¨∏ÏÑú, Ïù¥ÎØ∏ÏßÄ Îì±)Î•º **Îπ†Î•¥Í≤å Í≤ÄÏÉâ**ÌïòÍ∏∞ ÏúÑÌïú **ÌäπÌôîÎêú Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§**ÏûÖÎãàÎã§.

- **Ï£ºÏöî Í∏∞Îä•**  
  1. **Î≤°ÌÑ∞ ÏÇΩÏûÖ**  
     - ÏÇ¨Ï†Ñ ÌïôÏäµÎêú Î™®Îç∏(Ïòà: SentenceTransformer, BERT Îì±)Î°ú **ÌÖçÏä§Ìä∏/Ïù¥ÎØ∏ÏßÄ ‚Üí Î≤°ÌÑ∞**Î°ú Î≥ÄÌôòÌïòÏó¨ Ï†ÄÏû•  
  2. **Ïú†ÏÇ¨ÎèÑ Í≤ÄÏÉâ(ANN Search)**  
     - Approximate Nearest Neighbor Search Í∏∞Î≤ïÏùÑ ÌôúÏö©Ìï¥, ÎåÄÍ∑úÎ™® Î≤°ÌÑ∞ ÏßëÌï© ÎÇ¥ÏóêÏÑú **ÎπÑÏä∑Ìïú Î≤°ÌÑ∞(Î¨∏ÏÑú)**Î•º **Ìö®Ïú®Ï†ÅÏúºÎ°ú** Ï∞æÏùå  
  3. **ÌôïÏû•ÏÑ±**  
     - Îç∞Ïù¥ÌÑ∞Í∞Ä Îß§Ïö∞ ÎßéÏïÑÎèÑ **ÌôïÏû•(Scaling)**Í≥º Î∂ÑÏÇ∞ Ï≤òÎ¶¨Î•º ÌÜµÌï¥ **Îπ†Î•∏ Í≤ÄÏÉâ ÏÜçÎèÑ**Î•º Ïú†ÏßÄ

- **ÎåÄÌëúÏ†ÅÏù∏ Vector DB ÏòàÏãú**  
  - **FAISS**: Î©îÌÉÄ(Íµ¨ ÌéòÏù¥Ïä§Î∂Å)ÏóêÏÑú Í∞úÎ∞úÌïú Î≤°ÌÑ∞ Ïú†ÏÇ¨ÎèÑ Í≤ÄÏÉâ ÎùºÏù¥Î∏åÎü¨Î¶¨  
  - **Chroma**: Í∞úÏù∏ ÌîÑÎ°úÏ†ùÌä∏Î∂ÄÌÑ∞ ÎåÄÍ∑úÎ™® ÏÑúÎπÑÏä§ÍπåÏßÄ ÏâΩÍ≤å ÌôïÏû• Í∞ÄÎä•Ìïú Î≤°ÌÑ∞ DB  
  - **Milvus**: Í≥†ÏÑ±Îä•, ÎåÄÍ∑úÎ™® Î≤°ÌÑ∞ Í≤ÄÏÉâ ÏóîÏßÑ  
  - **Pinecone**: ÌÅ¥ÎùºÏö∞Îìú Í∏∞Î∞òÏùò ÏôÑÏ†ÑÍ¥ÄÎ¶¨Ìòï Î≤°ÌÑ∞ DB ÏÑúÎπÑÏä§

**ÌôúÏö© ÏÇ¨Î°Ä**  
- **RAG**(Retrieval-Augmented Generation)ÏóêÏÑú **Î¨∏ÏÑú Í≤ÄÏÉâ**  
- **Ïú†ÏÇ¨ÎèÑ Í∏∞Î∞ò Ï∂îÏ≤ú** ÏãúÏä§ÌÖú  
- Ïù¥ÎØ∏ÏßÄ Í≤ÄÏÉâ, ÏùåÏÑ± Í≤ÄÏÉâ Îì± **Î©ÄÌã∞Î™®Îã¨ Í≤ÄÏÉâ**

---

## 6. LLM Finetuning

**LLM Finetuning**ÏùÄ ÏÇ¨Ï†Ñ ÌïôÏäµÎêú ÎåÄÍ∑úÎ™® Ïñ∏Ïñ¥ Î™®Îç∏(Ïòà: GPT, BERT Îì±)ÏùÑ ÌäπÏ†ï ÌÉúÏä§ÌÅ¨ÎÇò ÎèÑÎ©îÏù∏Ïóê **ÎßûÏ∂§Ìôî**ÌïòÍ∏∞ ÏúÑÌï¥ Ï∂îÍ∞Ä ÌïôÏäµÌïòÎäî Í≥ºÏ†ïÏûÖÎãàÎã§.  
- Í∏∞Ï°¥ ÌååÎùºÎØ∏ÌÑ∞Î•º Ïû¨ÌôúÏö©ÌïòÎ©¥ÏÑú, **ÌäπÏ†ï Îç∞Ïù¥ÌÑ∞**Ïóê ÎåÄÌïú Î™®Îç∏ ÏÑ±Îä•ÏùÑ **ÌÅ¨Í≤å Í∞úÏÑ†**Ìï† Ïàò ÏûàÏäµÎãàÎã§.

### 6.1 ÌååÏù∏ÌäúÎãù Î∞©Î≤ï

1. **Ï†ÑÏ≤¥ ÌååÎùºÎØ∏ÌÑ∞ ÏóÖÎç∞Ïù¥Ìä∏(Full Finetuning)**  
   - Î™®Îç∏Ïùò Î™®Îì† ÌååÎùºÎØ∏ÌÑ∞Î•º ÎåÄÏÉÅÏúºÎ°ú, **ÏóêÌè¨ÌÅ¨(epoch)** Îã®ÏúÑÎ°ú **ÏµúÏ†ÅÌôî(Optimization)** ÏßÑÌñâ  
   - Îç∞Ïù¥ÌÑ∞ÏôÄ Í≥ÑÏÇ∞ ÏûêÏõêÏù¥ ÌíçÎ∂ÄÌïú Í≤ΩÏö∞Ïóê ÏÇ¨Ïö©

2. **PEFT(LoRA, Prefix Tuning Îì±)**  
   - **Î∂ÄÎ∂Ñ ÌååÎùºÎØ∏ÌÑ∞Îßå ÏóÖÎç∞Ïù¥Ìä∏**ÌïòÍ±∞ÎÇò, **Ï†ÄÎπÑÏö©**ÏúºÎ°ú Ï∂îÍ∞Ä ÌïôÏäµÌïòÎäî Î∞©Ïãù  
   - **LoRA(Low-Rank Adaptation)**: Î™®Îç∏ ÎÇ¥Î∂ÄÏùò ÌäπÏ†ï Í∞ÄÏ§ëÏπò ÌñâÎ†¨ÏùÑ **Ï†ÄÎû≠ÌÅ¨(Ï∂ïÏÜåÎêú Ï∞®Ïõê)** ÌòïÌÉúÎ°ú ÌïôÏäµÌï¥, **Ï†ÅÏùÄ Î©îÎ™®Î¶¨**Î°úÎèÑ ÌååÏù∏ÌäúÎãù Ìö®Í≥ºÎ•º ÏñªÏùÑ Ïàò ÏûàÏùå  
   - **Prefix Tuning**: ÏûÖÎ†• ÌÜ†ÌÅ∞ ÏïûÏóê **Í∞ÄÏÉÅ ÌîÑÎ°¨ÌîÑÌä∏(prefix)**Î•º Ï∂îÍ∞Ä ÌïôÏäµÌï¥, **Î™®Îç∏ Î≥∏Ï≤¥Îäî ÌÅ¨Í≤å Í±¥ÎìúÎ¶¨ÏßÄ ÏïäÍ≥†** ÏÑ±Îä• Ìñ•ÏÉÅÏùÑ Ïú†ÎèÑ

3. **ÌõàÎ†® ÎèÑÍµ¨**  
   - **Hugging Face Transformers** ÎùºÏù¥Î∏åÎü¨Î¶¨Ïùò **Trainer API**  
   - **Deepspeed**, **Accelerate** Îì± **Î∂ÑÏÇ∞ ÌõàÎ†®**, Î©îÎ™®Î¶¨ ÏµúÏ†ÅÌôî ÎùºÏù¥Î∏åÎü¨Î¶¨  
   - **PEFT ÎùºÏù¥Î∏åÎü¨Î¶¨**: LoRA Îì± Îã§ÏñëÌïú Í∏∞Î≤ïÏùÑ Í∞ÑÎã®ÌïòÍ≤å Ï†ÅÏö©

### 6.2 Ï£ºÏùòÏÇ¨Ìï≠

- **Îç∞Ïù¥ÌÑ∞ ÌíàÏßà**: ÌååÏù∏ÌäúÎãùÏóê ÏÇ¨Ïö©ÎêòÎäî Îç∞Ïù¥ÌÑ∞Ïùò **ÎèÑÎ©îÏù∏ Ï†ÅÌï©ÏÑ±**Í≥º **Î†àÏù¥Î∏î ÌíàÏßà**Ïù¥ Îß§Ïö∞ Ï§ëÏöî  
- **Ïò§Î≤ÑÌîºÌåÖ Î∞©ÏßÄ**: Î¨¥ÏûëÏ†ï ÌïôÏäµÎ•†Ïù¥ÎÇò ÏóêÌè¨ÌÅ¨Î•º ÎÜíÏù¥Î©¥, **ÏÉùÏÑ± Îä•Î†•Ïù¥ Îã®ÏàúÌôî**ÎêòÍ±∞ÎÇò **Î™®Îç∏ Ìé∏Ìñ•**Ïù¥ Î∞úÏÉùÌï† Ïàò ÏûàÏùå  
- **Î™®Îç∏ Ìò∏ÌôòÏÑ±**: ÏùºÎ∂Ä Î™®Îç∏ÏùÄ ÏïÑÌÇ§ÌÖçÏ≤ò ÌäπÏÑ±ÏÉÅ ÌååÏù∏ÌäúÎãùÏù¥ Ï†úÌïúÎê† Ïàò ÏûàÏúºÎØÄÎ°ú, **Í≥µÏãù Î¨∏ÏÑú**ÎÇò **Ïª§ÎÆ§ÎãàÌã∞ Ï†ïÎ≥¥**Î•º ÌôïÏù∏

---

Ïù¥Îü¨Ìïú **LangChain**, **RAG**, **Vector DB**, **LLM Finetuning** Í∏∞Î≤ïÎì§ÏùÑ Ï°∞Ìï©ÌïòÎ©¥,

1. **LLM ÌôúÏö© ÌååÏù¥ÌîÑÎùºÏù∏**ÏùÑ ÏÜêÏâΩÍ≤å Íµ¨ÏÑ±Ìï† Ïàò ÏûàÍ≥†,  
2. **Ï†ïÌôïÌïòÍ≥† ÌíçÎ∂ÄÌïú ÏßÄÏãù**ÏùÑ Í∏∞Î∞òÏúºÎ°ú Î™®Îç∏Ïùò ÏùëÎãµÏùÑ **Í∞ïÌôî**ÌïòÎ©∞,  
3. **ÌäπÏ†ï ÎèÑÎ©îÏù∏**Ïù¥ÎÇò **ÏóÖÎ¨¥ ÌôòÍ≤Ω**Ïóê ÏµúÏ†ÅÌôîÎêú **Ïª§Ïä§ÌÖÄ LLM**ÏùÑ Íµ¨Ï∂ïÌï† Ïàò ÏûàÏäµÎãàÎã§.

Ïã§Î¨¥ÎÇò Ïó∞Íµ¨ÏóêÏÑú LLMÏùÑ ÌôúÏö©Ìï† Îïå,  
- **LangChain**ÏúºÎ°ú **ÏõåÌÅ¨ÌîåÎ°úÏö∞**Î•º Ï≤¥Í≥ÑÌôîÌïòÍ≥†  
- **RAG**Î•º ÌÜµÌï¥ **Ïã§ÏãúÍ∞Ñ ÏßÄÏãù Í≤ÄÏÉâ**ÏùÑ ÎçîÌïòÎ©∞  
- **Vector DB**Î°ú **Í≤ÄÏÉâ ÏÑ±Îä•**ÏùÑ Í∑πÎåÄÌôîÌïòÍ≥†  
- **LLM Finetuning**ÏúºÎ°ú **ÎèÑÎ©îÏù∏ ÌäπÌôî Î™®Îç∏**ÏùÑ ÎßåÎì§Î©¥,  
ÎçîÏö± **Ìö®Ïú®Ï†Å**Ïù¥Í≥† **Í∞ïÎ†•**Ìïú NLP ÏÜîÎ£®ÏÖòÏùÑ Íµ¨ÌòÑÌï† Ïàò ÏûàÏùÑ Í≤ÉÏûÖÎãàÎã§.

---

## ÎÖ∏Ìä∏Î∂Å ÏãúÏó∞ ÎÇ¥Ïö©

Ïù¥ ÎÖ∏Ìä∏Î∂ÅÏóêÏÑúÎäî **Í∞ÑÎã®Ìïú ÏòàÏãú ÏΩîÎìú**Î•º ÌÜµÌï¥ Î°úÏª¨ ÌôòÍ≤ΩÏóêÏÑú Îã§ÏùåÏùÑ ÏÇ¥Ìé¥Î¥ÖÎãàÎã§:

1. **Î°úÏª¨ Llama Î™®Îç∏(ÎòêÎäî Ïú†ÏÇ¨ Î™®Îç∏) Î°úÎìú Î∞è Í∞ÑÎã®Ìïú Ï∂îÎ°†**  
2. **LangChain + Vector DBÎ•º Ïù¥Ïö©Ìïú RAG ÏòàÏãú**  
3. (**Í∞ÑÎã® Î≤ÑÏ†Ñ**) **LLM ÌååÏù∏ÌäúÎãù ÏòàÏãú**

pip install -r requirements.txt
  
ÌÑ∞ÎØ∏ÎÑêÏóêÏÑú Ïã§ÌñâÌïòÏó¨ ÌôòÍ≤Ω ÏÑ∏ÌåÖ

ÏòàÏãú: Î°úÏª¨ LLM Î°úÎìú
ÏïÑÎûò ÏòàÏãúÎäî Hugging Face Î™®Îç∏ Ï†ÄÏû•ÏÜåÎ°úÎ∂ÄÌÑ∞ Llama3.1-8bÎ•º Î°úÏª¨Ïóê Îã§Ïö¥Î°úÎìúÎ∞õÍ≥†,
Í∞ÑÎã® Ï∂îÎ°†ÏùÑ ÏàòÌñâÌïòÎäî ÏΩîÎìú ÏòàÏãúÏûÖÎãàÎã§.

Apple Ïã§Î¶¨ÏΩò(M1, M4 Îì±)ÏóêÏÑúÎäî PyTorch `mps` ÎîîÎ∞îÏù¥Ïä§Í∞Ä ÏûêÎèôÏúºÎ°ú Ïû°ÌûàÍ±∞ÎÇò ÏàòÎèô ÏÑ§Ï†ïÏù¥ ÌïÑÏöîÌï† Ïàò ÏûàÏäµÎãàÎã§.


---
ÌÜ†ÌÅ∞(Token)Í≥º ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä(Tokenizer)

ÏûêÏó∞Ïñ¥ Ï≤òÎ¶¨(NLP)ÏóêÏÑú **ÌÜ†ÌÅ∞Ìôî(Tokenization)**Îäî ÌÖçÏä§Ìä∏Î•º ÌäπÏ†ï Îã®ÏúÑÎ°ú Î∂ÑÌï†ÌïòÎäî Îß§Ïö∞ Ï§ëÏöîÌïú Ï†ÑÏ≤òÎ¶¨ Í≥ºÏ†ïÏûÖÎãàÎã§. Í∑∏ Í≥ºÏ†ïÏóêÏÑú ÌôúÏö©ÎêòÎäî **ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä(Tokenizer)**Ïóê ÎåÄÌï¥ ÏûêÏÑ∏Ìûà ÏïåÏïÑÎ≥¥Í≤†ÏäµÎãàÎã§.

1. ÌÜ†ÌÅ∞(Token)Ïù¥ÎûÄ?

**ÌÜ†ÌÅ∞(Token)**ÏùÄ ÌÖçÏä§Ìä∏(Î¨∏Ïû•, Î¨∏Îã® Îì±)Î•º ÏûëÏùÄ ÏùòÎØ∏ Îã®ÏúÑÎ°ú ÎÇòÎàà Í≤∞Í≥ºÎ¨ºÏûÖÎãàÎã§. Ïñ¥Îñ§ Í∏∞Ï§ÄÏúºÎ°ú ÎÇòÎàÑÎäêÎÉêÏóê Îî∞Îùº Îã§ÏñëÌïú ÌÜ†ÌÅ∞ Îã®ÏúÑÎ•º ÏñªÏùÑ Ïàò ÏûàÏäµÎãàÎã§.

1.1 Îã®Ïñ¥ Îã®ÏúÑ
	‚Ä¢	Í≥µÎ∞±(whitespace)Ïù¥ÎÇò Íµ¨ÎëêÏ†ê(punctuation)ÏùÑ Í∏∞Ï§ÄÏúºÎ°ú Î∂ÑÎ¶¨ÌïòÎäî Í∞ÑÎã®Ìïú Î∞©Ïãù
	‚Ä¢	Ïòà) ‚ÄúÎÇòÎäî ÌïôÍµêÏóê Í∞ÑÎã§.‚Äù ‚Üí ["ÎÇòÎäî", "ÌïôÍµêÏóê", "Í∞ÑÎã§."]

1.2 ÌòïÌÉúÏÜå Îã®ÏúÑ
	‚Ä¢	ÌïúÍµ≠Ïñ¥ÏóêÏÑú ÏûêÏ£º ÏÇ¨Ïö©ÌïòÎäî Î∞©Ïãù
	‚Ä¢	Ïòà) ‚ÄúÎÇòÎäî ÌïôÍµêÏóê Í∞ÑÎã§.‚Äù ‚Üí ["ÎÇò", "Îäî", "ÌïôÍµê", "Ïóê", "Í∞Ä", "·ÑÇÎã§", "."]

1.3 ÏÑúÎ∏åÏõåÎìú(Subword) Îã®ÏúÑ
	‚Ä¢	BPE, WordPiece, SentencePiece Îì±Ïùò ÏïåÍ≥†Î¶¨Ï¶òÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ Îã®Ïñ¥Î≥¥Îã§ ÏûëÏùÄ Îã®ÏúÑÎ°ú ÎÇòÎàî
	‚Ä¢	Ïòà) ‚Äúunhappy‚Äù ‚Üí ["un", "happy"]
	‚Ä¢	Ïòà) ‚Äúunbelievable‚Äù ‚Üí ["un", "believable"]

2. ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä(Tokenizer)ÎûÄ?

**ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä(Tokenizer)**Îäî ÌÖçÏä§Ìä∏Î•º ÌäπÏ†ï Í∑úÏπô ÌòπÏùÄ ÏïåÍ≥†Î¶¨Ï¶òÏùÑ ÏÇ¨Ïö©Ìï¥ ÌÜ†ÌÅ∞ÏúºÎ°ú Î∂ÑÌï†ÌïòÎäî ÎèÑÍµ¨(ÎòêÎäî ÎùºÏù¥Î∏åÎü¨Î¶¨)ÏûÖÎãàÎã§.

2.1 Í∑úÏπô Í∏∞Î∞ò ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä
	‚Ä¢	Í≥µÎ∞±, Íµ¨ÎëêÏ†ê, ÌäπÏ†ï Ìå®ÌÑ¥(Ï†ïÍ∑ú ÌëúÌòÑÏãù) Îì±ÏùÑ Í∏∞Ï§ÄÏúºÎ°ú ÎØ∏Î¶¨ Ï†ïÌï¥ÏßÑ Í∑úÏπôÏóê Îî∞Îùº Î∂ÑÎ¶¨
	‚Ä¢	Ïòà) split(), Ï†ïÍ∑úÏãù(Regex)ÏùÑ ÌÜµÌïú Îã®Ïàú Î∂ÑÌï†

2.2 ÌïôÏäµ Í∏∞Î∞ò ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä
	‚Ä¢	ÌõàÎ†® Îç∞Ïù¥ÌÑ∞(ÏΩîÌçºÏä§)Ïóê ÎßûÏ∂∞ ÌÜ†ÌÅ∞ ÏÇ¨Ï†ÑÏùÑ ÏûêÎèô ÏÉùÏÑ±ÌïòÍ≥†, Ìï¥Îãπ Í∑úÏπôÏùÑ ÌïôÏäµ
	‚Ä¢	BPE(Byte-Pair Encoding), WordPiece, SentencePiece Îì±Ïù¥ ÎåÄÌëúÏ†Å
	‚Ä¢	ÎåÄÍ∑úÎ™® Ïñ∏Ïñ¥ Î™®Îç∏(Ïòà: BERT, GPT Îì±)ÏóêÏÑúÎèÑ ÏÇ¨Ïö©

3. ÌÜ†ÌÅ∞Ìôî Í≥ºÏ†ïÏù¥ Ï§ëÏöîÌïú Ïù¥Ïú†
	1.	Ï†ïÌôïÎèÑ Ìñ•ÏÉÅ
	‚Ä¢	Ïò¨Î∞îÎ•∏ Îã®ÏúÑÎ°ú ÌÖçÏä§Ìä∏Î•º ÎÇòÎàÑÏñ¥Ïïº ÌòïÌÉúÏÜå Î∂ÑÏÑù, ÌíàÏÇ¨ ÌÉúÍπÖ Îì± ÌõÑÏÜç ÏûëÏóÖÏùò Ï†ïÌôïÎèÑÍ∞Ä ÎÜíÏïÑÏßëÎãàÎã§.
	2.	Ïñ¥Ìúò ÏÇ¨Ï†Ñ Í¥ÄÎ¶¨
	‚Ä¢	Îã®Ïñ¥ Îã®ÏúÑÎ°úÎßå ÎÇòÎàÑÎ©¥ Ïñ¥Ìúò ÏÇ¨Ï†ÑÏù¥ ÎÑàÎ¨¥ Ïª§Ïßà Ïàò ÏûàÏúºÎÇò, ÏÑúÎ∏åÏõåÎìú Î∞©ÏãùÏùÑ ÏÇ¨Ïö©ÌïòÎ©¥ Ìù¨Í∑Ä Îã®Ïñ¥ÎÇò Ïã†Ï°∞Ïñ¥ÎèÑ Ìö®Ïú®Ï†ÅÏúºÎ°ú Ï≤òÎ¶¨Ìï† Ïàò ÏûàÏäµÎãàÎã§.
	3.	Î™®Îç∏ ÏÑ±Îä• Í∑πÎåÄÌôî
	‚Ä¢	ÌÜ†ÌÅ∞ÌôîÍ∞Ä ÏûòÎ™ªÎêòÎ©¥ Î™®Îç∏Ïù¥ ÌïôÏäµÏóê Ïñ¥Î†§ÏõÄÏùÑ Í≤™Í±∞ÎÇò, Ï∂îÎ°† Ïãú ÏÑ±Îä•Ïù¥ Îñ®Ïñ¥ÏßëÎãàÎã§.
	‚Ä¢	BERTÎÇò GPT Í∞ôÏùÄ Î™®Îç∏Îì§ÎèÑ ÏùºÍ¥ÄÎêú ÌÜ†ÌÅ¨ÎÇòÏù¥Ïßï Í∑úÏπôÏùÑ Í∏∞Î∞òÏúºÎ°ú ÌïôÏäµÎê©ÎãàÎã§.

4. ÌïúÍµ≠Ïñ¥ ÌÜ†ÌÅ¨ÎÇòÏù¥Ïßï Ïãú Í≥†Î†§ÏÇ¨Ìï≠
	1.	ÌòïÌÉúÏÜå Î∂ÑÏÑùÏùò ÌïÑÏöîÏÑ±
	‚Ä¢	ÌïúÍµ≠Ïñ¥Îäî Ï°∞ÏÇ¨, Ïñ¥ÎØ∏ Î≥ÄÌôî Îì±Ïù¥ Îã§ÏñëÌï¥ Îã®Ïàú Í≥µÎ∞±ÏúºÎ°úÎßå ÌÜ†ÌÅ∞ÌôîÌïòÍ∏∞ Ïñ¥Î†µÏäµÎãàÎã§.
	2.	Ïñ¥Ï†à ÎÇ¥Î∂ÄÏùò Îã§ÏñëÌïú ÏöîÏÜå
	‚Ä¢	‚ÄúÌïôÍµêÏóê‚Äù ‚Üí ["ÌïôÍµê", "Ïóê"]
	‚Ä¢	‚ÄúÌïôÍµêÏóêÏÑú‚Äù ‚Üí ["ÌïôÍµê", "ÏóêÏÑú"]
	‚Ä¢	Ï°∞ÏÇ¨ÏôÄ Ïñ¥Í∞Ñ Îì±ÏùÑ Î∂ÑÎ¶¨Ìï¥Ï§òÏïº ÏõêÌïòÎäî Ï†ïÌôïÎèÑÎ•º ÏñªÏùÑ Ïàò ÏûàÏäµÎãàÎã§.
	3.	Î∂àÍ∑úÏπôÌïú ÎùÑÏñ¥Ïì∞Í∏∞
	‚Ä¢	ÎùÑÏñ¥Ïì∞Í∏∞Î•º Ï†ïÌôïÌûà ÏßÄÌÇ§ÏßÄ ÏïäÎäî Í≤ΩÏö∞Í∞Ä ÎßéÏúºÎØÄÎ°ú Í∑úÏπô Í∏∞Î∞òÎßåÏúºÎ°úÎäî Ï≤òÎ¶¨Ïóê ÌïúÍ≥ÑÍ∞Ä ÏûàÏäµÎãàÎã§.

5. ÎåÄÌëúÏ†ÅÏù∏ ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä ÏòàÏãú
	‚Ä¢	NLTK (ÏòÅÏñ¥)
	‚Ä¢	ÌååÏù¥Ïç¨ÏóêÏÑú Í∞ÄÏû• ÎßéÏù¥ ÏÇ¨Ïö©ÎêòÎäî NLP ÎùºÏù¥Î∏åÎü¨Î¶¨ Ï§ë ÌïòÎÇò
	‚Ä¢	Îã®Ïñ¥ ÌÜ†ÌÅ∞Ìôî, Î¨∏Ïû• ÌÜ†ÌÅ∞Ìôî, Ïä§ÌÜ±ÏõåÎìú Ï†úÍ±∞ Îì± Îã§ÏñëÌïú Í∏∞Îä• Ï†úÍ≥µ
	‚Ä¢	KoNLPy (ÌïúÍµ≠Ïñ¥)
	‚Ä¢	Ìä∏ÏúÑÌÑ∞(Twitter), Íº¨Íº¨Îßà(Kkma), ÌïúÎÇòÎàî(Hannanum) Îì± Ïó¨Îü¨ ÌòïÌÉúÏÜå Î∂ÑÏÑùÍ∏∞ Ïó∞Îèô Í∞ÄÎä•
	‚Ä¢	ÌòïÌÉúÏÜå Î∂ÑÏÑù, ÌíàÏÇ¨ ÌÉúÍπÖ Îì± ÌïúÍµ≠Ïñ¥ Ï†ÑÏö© Ï≤òÎ¶¨ Í∏∞Îä•
	‚Ä¢	BPE, SentencePiece, WordPiece
	‚Ä¢	ÏÑúÎ∏åÏõåÎìú(subword) Í∏∞Î∞ò ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä
	‚Ä¢	ÎåÄÍ∑úÎ™® Ïñ∏Ïñ¥ Î™®Îç∏(Ïòà: BERT, GPT, RoBERTa Îì±)ÏóêÏÑú ÏÇ¨Ïö©

6. Ï†ïÎ¶¨
	‚Ä¢	ÌÜ†ÌÅ∞(Token)
	‚Ä¢	ÌÖçÏä§Ìä∏Î•º ÏûëÏùÄ Îã®ÏúÑ(Îã®Ïñ¥, ÌòïÌÉúÏÜå, ÏÑúÎ∏åÏõåÎìú Îì±)Î°ú Î∂ÑÌï†Ìïú Í≤∞Í≥ºÎ¨º
	‚Ä¢	ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä(Tokenizer)
	‚Ä¢	ÌÖçÏä§Ìä∏Î•º ÏõêÌïòÎäî Í∏∞Ï§ÄÏúºÎ°ú Î∂ÑÌï†ÌïòÍ∏∞ ÏúÑÌïú ÎèÑÍµ¨ ÎòêÎäî ÏïåÍ≥†Î¶¨Ï¶ò
	‚Ä¢	ÌÜ†ÌÅ∞Ìôî(Tokenization)
	‚Ä¢	ÏûêÏó∞Ïñ¥ Ï≤òÎ¶¨Ïùò ÌïµÏã¨ Ï†ÑÏ≤òÎ¶¨ Îã®Í≥ÑÏù¥Î©∞, Ïò¨Î∞îÎ•∏ ÌÜ†ÌÅ∞ÌôîÎ•º ÌÜµÌï¥ Î™®Îç∏Ïùò ÏÑ±Îä•ÏùÑ ÎÜíÏù¥Í≥† Ïñ¥Ìúò ÏÇ¨Ï†ÑÏùÑ Ìö®Ïú®Ï†ÅÏúºÎ°ú Í¥ÄÎ¶¨Ìï† Ïàò ÏûàÏùå
	‚Ä¢	ÌïúÍµ≠Ïñ¥Îäî Ï°∞ÏÇ¨, Ïñ¥ÎØ∏ Î≥ÄÌôî Îì±Ïù¥ Î≥µÏû°ÌïòÎØÄÎ°ú ÌòïÌÉúÏÜå Î∂ÑÏÑù ÎòêÎäî ÏÑúÎ∏åÏõåÎìú Î∞©ÏãùÏùÑ Í≥†Î†§ÌïòÎäî Í≤ÉÏù¥ ÏùºÎ∞òÏ†Å

Í≤∞Î°†Ï†ÅÏúºÎ°ú, Ï†ÅÏ†àÌïú ÌÜ†ÌÅ¨ÎÇòÏù¥ÏßïÏùÑ ÌÜµÌï¥ Îç∞Ïù¥ÌÑ∞ ÌíàÏßàÍ≥º Î™®Îç∏ ÏÑ±Îä•ÏùÑ ÌÅ¨Í≤å Ìñ•ÏÉÅÏãúÌÇ¨ Ïàò ÏûàÏäµÎãàÎã§. NLP ÌîÑÎ°úÏ†ùÌä∏Ïóê ÎßûÎäî ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†ÄÎ•º ÏÑ†ÌÉùÌïòÍ≥†, Ïñ∏Ïñ¥Î≥Ñ ÌäπÏÑ±ÏùÑ Î∞òÏòÅÌïú ÌÜ†ÌÅ∞Ìôî Ï†ÑÎûµÏùÑ ÏàòÎ¶ΩÌïòÎäî Í≤ÉÏù¥ Ï§ëÏöîÌï©ÎãàÎã§.


```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# Llama Î™®Îç∏ ÏòàÏãú (Ïã§Ï†ú ÏÇ¨Ïö© Ïãú, Hugging FaceÏóêÏÑú Ï†ëÍ∑º Í∂åÌïúÏù¥ ÌïÑÏöîÌïòÍ±∞ÎÇò Î™®Îç∏ Ïù¥Î¶ÑÏù¥ Îã§Î•º Ïàò ÏûàÏäµÎãàÎã§.)
#model_name = "meta-llama/Llama-3.1-8B"
model_name="../models/Llama-3.1-8b"  
# mps ÎîîÎ∞îÏù¥Ïä§ ÏÇ¨Ïö© Í∞ÄÎä• Ïó¨Î∂Ä ÌôïÏù∏
device = torch.device("mps") if torch.backends.mps.is_available() else torch.device("cpu")
print("Using device:", device)

# ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†ÄÏôÄ Î™®Îç∏ Î°úÎìú
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map={"": device})

# Í∞ÑÎã®Ìïú ÌÖçÏä§Ìä∏ ÏÉùÏÑ± ÏòàÏãú
prompt = "Hello, how are you?"
inputs = tokenizer(prompt, return_tensors="pt").to(device)
with torch.no_grad():
    outputs = model.generate(**inputs, max_new_tokens=50)
generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
print("Generated text:")
print(generated_text)

```

    Using device: mps



    Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]


    Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.


    Generated text:
    Hello, how are you? Today we are going to talk about the new generation of smart watches, which are very popular in the world of technology and are very useful for our daily life.
    The smart watch is a device that is in charge of tracking the physical activity of its user


ÏÑ§Î™Ö:

- torch_dtype=torch.float16: Apple MPSÏóêÏÑúÎäî float16Ïù¥ ÏµúÏ†ÅÏûÖÎãàÎã§.
- device_map={"": device}: Î™®Îç∏ÏùÑ ÏûêÎèôÏúºÎ°ú MPS(GPU) ÎòêÎäî CPUÏóê Î∞∞ÏπòÌï©ÎãàÎã§.
- max_new_tokens=50: ÏùëÎãµ Í∏∏Ïù¥Î•º 50Í∞ú ÌÜ†ÌÅ∞ÏúºÎ°ú Ï†úÌïúÌï©ÎãàÎã§.

Llama 3.1 8B Î™®Îç∏ + LangChain + Vector DB(RAG) Ï†ÅÏö©
Llama 3.1 8B Î™®Îç∏ÏùÑ LangChain Î∞è ChromaDB(Vector DB)ÏôÄ Ìï®Íªò ÏÇ¨Ïö©ÌïòÏó¨ Retrieval-Augmented Generation(RAG) ÏùÑ ÏàòÌñâÌï† ÏàòÎèÑ ÏûàÏäµÎãàÎã§.

2.1 LangChain & ChromaDB ÏÑ§Ïπò

```bash
pip install langchain chromadb faiss-cpu sentence-transformers
```

2.2 RAG Ï†ÅÏö© ÏΩîÎìú


```python
# ÌïÑÏöîÌïú ÎùºÏù¥Î∏åÎü¨Î¶¨ ÏûÑÌè¨Ìä∏
from transformers import pipeline  # <-- Ïó¨Í∏∞Ïóê Ï∂îÍ∞Ä
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.docstore.document import Document
from langchain.chains import RetrievalQA
from langchain.llms import HuggingFacePipeline

# 1) ÏûÑÎ≤†Îî© Î™®Îç∏ Î°úÎìú
embedding_model = "sentence-transformers/all-MiniLM-L6-v2"
embeddings = HuggingFaceEmbeddings(model_name=embedding_model)

# ÏòàÏ†ú Î¨∏ÏÑú
texts = [
    "LangChainÏùÄ LLMÏùÑ Ï≤¥Ïù∏ÏúºÎ°ú Ïó∞Í≤∞ÌïòÎäî ÌîÑÎ†àÏûÑÏõåÌÅ¨ÏûÖÎãàÎã§.",
    "RAGÎäî Retrieval-Augmented GenerationÏùò ÏïΩÏñ¥ÏûÖÎãàÎã§.",
    "Vector DBÎäî Î¨∏ÏÑú ÏûÑÎ≤†Îî© Î≤°ÌÑ∞Î•º Í≤ÄÏÉâÌïòÍ∏∞ ÏúÑÌïú Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ÏûÖÎãàÎã§."
]
documents = [Document(page_content=t) for t in texts]

# 2) Chroma VectorStore Ï¥àÍ∏∞Ìôî
vectorstore = Chroma.from_documents(documents, embedding=embeddings, collection_name="example_collection")

# 3) Llama 3.1 8B Î™®Îç∏ÏùÑ LangChainÏóê Ïó∞Í≤∞
generator_pipeline = pipeline(
    "text-generation",
    model=model,  # Llama 3.1 8B Î™®Îç∏
    tokenizer=tokenizer
)
llm = HuggingFacePipeline(pipeline=generator_pipeline)

# 4) RetrievalQA Ï≤¥Ïù∏ ÏÉùÏÑ±
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=vectorstore.as_retriever()
)

# 5) ÏßàÎ¨∏ ÏûÖÎ†• Î∞è RAG Í∏∞Î∞ò ÎãµÎ≥Ä ÏÉùÏÑ±
query = "RAGÎäî Î¨¥ÏóáÏù∏Í∞ÄÏöî?"
answer = qa_chain.run(query)
print(f"Q: {query}\nA: {answer}")

```

    Device set to use mps
    /var/folders/z8/94fh0xbx5cv85y4f8dm4_nch0000gn/T/ipykernel_1491/842959494.py:30: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.
      llm = HuggingFacePipeline(pipeline=generator_pipeline)
    /var/folders/z8/94fh0xbx5cv85y4f8dm4_nch0000gn/T/ipykernel_1491/842959494.py:41: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.
      answer = qa_chain.run(query)
    Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.


    Q: RAGÎäî Î¨¥ÏóáÏù∏Í∞ÄÏöî?
    A: Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.
    
    RAGÎäî Retrieval-Augmented GenerationÏùò ÏïΩÏñ¥ÏûÖÎãàÎã§.
    
    RAGÎäî Retrieval-Augmented GenerationÏùò ÏïΩÏñ¥ÏûÖÎãàÎã§.
    
    RAGÎäî Retrieval-Augmented GenerationÏùò ÏïΩÏñ¥ÏûÖÎãàÎã§.
    
    LangChainÏùÄ LLMÏùÑ Ï≤¥Ïù∏ÏúºÎ°ú Ïó∞Í≤∞ÌïòÎäî ÌîÑÎ†àÏûÑÏõåÌÅ¨ÏûÖÎãàÎã§.
    
    Question: RAGÎäî Î¨¥ÏóáÏù∏Í∞ÄÏöî?
    Helpful Answer: Retrieval-Augmented GenerationÏùò ÏïΩÏñ¥ÏûÖÎãàÎã§.


## **ÏûÑÎ≤†Îî© Î™®Îç∏(Embedding Model)Ïù¥ÎûÄ?**

### 1Ô∏è‚É£ **Í∞úÎÖê**
ÏûÑÎ≤†Îî© Î™®Îç∏(Embedding Model)Ïù¥ÎûÄ, Îç∞Ïù¥ÌÑ∞Î•º **Í≥†Ï†ïÎêú ÌÅ¨Í∏∞Ïùò Î≤°ÌÑ∞(vector) ÌòïÌÉúÎ°ú Î≥ÄÌôòÌïòÎäî Î™®Îç∏**ÏùÑ ÏùòÎØ∏Ìï©ÎãàÎã§.  
ÌÖçÏä§Ìä∏, Ïù¥ÎØ∏ÏßÄ, Ïò§ÎîîÏò§ Îì± Îã§ÏñëÌïú Îç∞Ïù¥ÌÑ∞ Ïú†ÌòïÏùÑ **ÏàòÏπò Î≤°ÌÑ∞Î°ú Î≥ÄÌôò**ÌïòÏó¨ Ïª¥Ìì®ÌÑ∞Í∞Ä Ïù¥Ìï¥ÌïòÍ≥† Ïó∞ÏÇ∞Ìï† Ïàò ÏûàÎèÑÎ°ù ÎßåÎì§Ïñ¥ Ï§çÎãàÎã§.

Ï¶â, **ÏûÑÎ≤†Îî©(Embedding)**ÏùÄ Í≥†Ï∞®Ïõê Îç∞Ïù¥ÌÑ∞(Ïòà: Îã®Ïñ¥, Î¨∏Ïû•, Î¨∏ÏÑú, Ïù¥ÎØ∏ÏßÄ Îì±)Î•º **Î∞ÄÏßë Î≤°ÌÑ∞(Dense Vector)**Î°ú Î≥ÄÌôòÌïòÎäî Í≥ºÏ†ïÏù¥Î©∞,  
Ïù¥ Î≤°ÌÑ∞Î•º ÏÉùÏÑ±ÌïòÎäî Î™®Îç∏ÏùÑ **ÏûÑÎ≤†Îî© Î™®Îç∏(Embedding Model)**Ïù¥ÎùºÍ≥† Ìï©ÎãàÎã§.

---

### 2Ô∏è‚É£ **Ïôú ÏûÑÎ≤†Îî©Ïù¥ ÌïÑÏöîÌïúÍ∞Ä?**
Ïª¥Ìì®ÌÑ∞Îäî Ïà´ÏûêÎßå Ïù¥Ìï¥Ìï† Ïàò ÏûàÏúºÎØÄÎ°ú, ÏûêÏó∞Ïñ¥(NLP)ÎÇò Ïù¥ÎØ∏ÏßÄ Îç∞Ïù¥ÌÑ∞Î•º ÏßÅÏ†ë Ï≤òÎ¶¨Ìï† Ïàò ÏóÜÏäµÎãàÎã§.  
Îî∞ÎùºÏÑú, Îã§ÏùåÍ≥º Í∞ôÏùÄ Ïù¥Ïú†Î°ú **ÏûÑÎ≤†Îî© Î™®Îç∏**Ïù¥ ÌïÑÏöîÌï©ÎãàÎã§.

‚úÖ **Î¨∏ÏûêÏó¥ÏùÑ Ïà´ÏûêÎ°ú Î≥ÄÌôò**: ÌÖçÏä§Ìä∏Î•º Ïà´Ïûê Î≤°ÌÑ∞Î°ú Î≥ÄÌôòÌï¥Ïïº Î®∏Ïã†Îü¨Îãù Î™®Îç∏Ïù¥ Ïù¥Ìï¥Ìï† Ïàò ÏûàÏùå  
‚úÖ **Ïú†ÏÇ¨Ìïú Îç∞Ïù¥ÌÑ∞ÎÅºÎ¶¨ Í∞ÄÍπåÏö¥ Î≤°ÌÑ∞ Í∞íÏùÑ Í∞ÄÏßê**: ÏùòÎØ∏Ï†ÅÏúºÎ°ú ÎπÑÏä∑Ìïú Îã®Ïñ¥Îäî Ïú†ÏÇ¨Ìïú Î≤°ÌÑ∞Î°ú Î≥ÄÌôòÎê®  
‚úÖ **Í≥†Ï∞®Ïõê Îç∞Ïù¥ÌÑ∞Î•º Ï†ÄÏ∞®ÏõêÏúºÎ°ú ÏïïÏ∂ï**: Ï∞®ÏõêÏù¥ ÎÜíÏùÄ Îç∞Ïù¥ÌÑ∞Î•º ÎÇÆÏùÄ Ï∞®ÏõêÏùò Î≤°ÌÑ∞Î°ú Î≥ÄÌôòÌïòÏó¨ Ïó∞ÏÇ∞ÏùÑ ÏµúÏ†ÅÌôî  

> ÏòàÎ•º Îì§Ïñ¥, "Í≥†ÏñëÏù¥(cat)"ÏôÄ "Í∞ïÏïÑÏßÄ(dog)"Îäî **ÎπÑÏä∑Ìïú ÏùòÎØ∏**Î•º Í∞ÄÏßÄÎØÄÎ°ú, ÏûÑÎ≤†Îî© Î≤°ÌÑ∞ Í≥µÍ∞ÑÏóêÏÑú ÏÑúÎ°ú Í∞ÄÍπåÏö¥ ÏúÑÏπòÏóê ÏûàÏùå.  
> Î∞òÎ©¥, "Í≥†ÏñëÏù¥(cat)"ÏôÄ "ÏûêÎèôÏ∞®(car)"Îäî Í¥ÄÎ†®ÏÑ±Ïù¥ Ï†ÅÏúºÎØÄÎ°ú Î≤°ÌÑ∞ Í≥µÍ∞ÑÏóêÏÑú Î©ÄÎ¶¨ Îñ®Ïñ¥Ï†∏ ÏûàÏùå.

---

### 3Ô∏è‚É£ **ÏûÑÎ≤†Îî© Î™®Îç∏Ïùò Ï¢ÖÎ•ò**
ÏûÑÎ≤†Îî© Î™®Îç∏ÏùÄ **ÌÖçÏä§Ìä∏, Ïù¥ÎØ∏ÏßÄ, Ïò§ÎîîÏò§** Îì± Îã§ÏñëÌïú Îç∞Ïù¥ÌÑ∞Î•º Î≤°ÌÑ∞ÌôîÌïòÎäî Îç∞ ÏÇ¨Ïö©Îê©ÎãàÎã§.  
ÎåÄÌëúÏ†ÅÏù∏ ÏûÑÎ≤†Îî© Î™®Îç∏ Ï¢ÖÎ•òÎäî Îã§ÏùåÍ≥º Í∞ôÏäµÎãàÎã§.

#### üîπ **(1) ÌÖçÏä§Ìä∏ ÏûÑÎ≤†Îî© (Word/Sentence Embedding)**
- ÏûêÏó∞Ïñ¥ Ï≤òÎ¶¨(NLP)ÏóêÏÑú Îã®Ïñ¥, Î¨∏Ïû•, Î¨∏ÏÑúÎ•º Î≤°ÌÑ∞Î°ú Î≥ÄÌôòÌïòÎäî Î™®Îç∏
- **ÏÇ¨Ïö©Ï≤ò**: Ï±óÎ¥á, Í≤ÄÏÉâ, Ï∂îÏ≤ú ÏãúÏä§ÌÖú, Î¨∏ÏÑú Î∂ÑÎ•ò, RAG(Retrieval-Augmented Generation)

üìå **ÎåÄÌëúÏ†ÅÏù∏ Î™®Îç∏**
- `Word2Vec`: Îã®Ïñ¥Î•º Î≤°ÌÑ∞ÌôîÌïòÎäî ÎåÄÌëúÏ†ÅÏù∏ Î™®Îç∏
- `GloVe(Global Vectors for Word Representation)`: Îã®Ïñ¥ Í∞ÑÏùò ÌÜµÍ≥ÑÏ†Å Ïó∞Í¥ÄÏÑ±ÏùÑ Î∞òÏòÅÌïú Î≤°ÌÑ∞
- `FastText`: Word2Vec Í∞úÏÑ† Î≤ÑÏ†Ñ (Î∂ÄÎ∂Ñ Îã®Ïñ¥ÍπåÏßÄ ÏûÑÎ≤†Îî© Í∞ÄÎä•)
- `BERT Embedding`: Î¨∏Îß•ÏùÑ Í≥†Î†§ÌïòÏó¨ Îã®Ïñ¥/Î¨∏Ïû•ÏùÑ Î≤°ÌÑ∞Î°ú Î≥ÄÌôòÌïòÎäî Í∞ïÎ†•Ìïú Î™®Îç∏
- `Sentence-BERT (SBERT)`: Î¨∏Ïû• Îã®ÏúÑ ÏûÑÎ≤†Îî©ÏùÑ ÏÉùÏÑ±ÌïòÎäî Î™®Îç∏
- `sentence-transformers/all-MiniLM-L6-v2`: Í∞ÄÎ≥çÍ≥† Îπ†Î•∏ Î¨∏Ïû• ÏûÑÎ≤†Îî© Î™®Îç∏ (LangChain, RAGÏóêÏÑú ÎßéÏù¥ ÏÇ¨Ïö©Îê®)

‚úÖ **ÏòàÏ†ú**: `sentence-transformers`Î•º ÏÇ¨Ïö©ÌïòÏó¨ ÌÖçÏä§Ìä∏ ÏûÑÎ≤†Îî© Î≤°ÌÑ∞ ÏÉùÏÑ±  
```python
from sentence_transformers import SentenceTransformer

# SBERT Í∏∞Î∞ò Î¨∏Ïû• ÏûÑÎ≤†Îî© Î™®Îç∏ Î°úÎìú
model = SentenceTransformer("all-MiniLM-L6-v2")

# ÏòàÏ†ú Î¨∏Ïû•
sentences = ["Í≥†ÏñëÏù¥Îäî Í∑ÄÏóΩÎã§.", "Í∞ïÏïÑÏßÄÎäî Ï∂©ÏÑ±Ïä§ÎüΩÎã§.", "ÏûêÎèôÏ∞®Îäî Îπ†Î•¥Îã§."]

# ÏûÑÎ≤†Îî© Î≥ÄÌôò (Î≤°ÌÑ∞ ÏÉùÏÑ±)
embeddings = model.encode(sentences)

# Ï∂úÎ†• (Ï≤´ Î≤àÏß∏ Î¨∏Ïû•Ïùò Î≤°ÌÑ∞ Í∞í)
print(embeddings[0])
```

---

#### üîπ **(2) Ïù¥ÎØ∏ÏßÄ ÏûÑÎ≤†Îî© (Image Embedding)**
- Ïù¥ÎØ∏ÏßÄ Îç∞Ïù¥ÌÑ∞Î•º Î≤°ÌÑ∞Î°ú Î≥ÄÌôòÌïòÏó¨ ÎπÑÏä∑Ìïú Ïù¥ÎØ∏ÏßÄ Í≤ÄÏÉâ, Í∞ùÏ≤¥ Ïù∏Ïãù Îì±Ïóê ÌôúÏö©
- **ÏÇ¨Ïö©Ï≤ò**: Ïù¥ÎØ∏ÏßÄ Í≤ÄÏÉâ ÏãúÏä§ÌÖú, Ïä§ÌÉÄÏùº Ï∂îÏ≤ú, Ïª¥Ìì®ÌÑ∞ ÎπÑÏ†Ñ

üìå **ÎåÄÌëúÏ†ÅÏù∏ Î™®Îç∏**
- `ResNet-50`, `EfficientNet`, `CLIP`: Ïù¥ÎØ∏ÏßÄ Î∂ÑÎ•ò Î∞è ÌäπÏßï Î≤°ÌÑ∞ Ï∂îÏ∂ú
- `DINOv2`: ÏµúÍ∑º MetaÏóêÏÑú Î∞úÌëúÌïú Ïù¥ÎØ∏ÏßÄ ÏûÑÎ≤†Îî© Î™®Îç∏
- `OpenAI CLIP`: ÌÖçÏä§Ìä∏ÏôÄ Ïù¥ÎØ∏ÏßÄÎ•º ÎèôÏùºÌïú Î≤°ÌÑ∞ Í≥µÍ∞ÑÏóê Îß§ÌïëÌïòÎäî Î™®Îç∏ (Ïòà: "Í∞ïÏïÑÏßÄ ÏÇ¨ÏßÑ" ‚Üí Í∞ïÏïÑÏßÄ Ïù¥ÎØ∏ÏßÄ Î≤°ÌÑ∞ÏôÄ Ïú†ÏÇ¨Ìïú ÏúÑÏπò)

‚úÖ **ÏòàÏ†ú**: `CLIP`ÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ Ïù¥ÎØ∏ÏßÄ ÏûÑÎ≤†Îî© Î≤°ÌÑ∞ ÏÉùÏÑ±
```python
import torch
import clip
from PIL import Image

# CLIP Î™®Îç∏ Î°úÎìú
device = "cuda" if torch.cuda.is_available() else "cpu"
model, preprocess = clip.load("ViT-B/32", device=device)

# Ïù¥ÎØ∏ÏßÄ Î°úÎìú Î∞è Î≥ÄÌôò
image = preprocess(Image.open("dog.jpg")).unsqueeze(0).to(device)

# ÏûÑÎ≤†Îî© Î≤°ÌÑ∞ Ï∂îÏ∂ú
with torch.no_grad():
    image_features = model.encode_image(image)

print(image_features.shape)  # [1, 512] ÌÅ¨Í∏∞Ïùò Î≤°ÌÑ∞ Ï∂úÎ†•
```

---

#### üîπ **(3) Ïò§ÎîîÏò§ ÏûÑÎ≤†Îî© (Audio Embedding)**
- Ïò§ÎîîÏò§ Îç∞Ïù¥ÌÑ∞Î•º Î≤°ÌÑ∞Î°ú Î≥ÄÌôòÌïòÏó¨ ÏùåÏÑ± Ïù∏Ïãù, Í∞êÏ†ï Î∂ÑÏÑù Îì±Ïóê ÌôúÏö©
- **ÏÇ¨Ïö©Ï≤ò**: ÏùåÏÑ± Ïù∏Ïãù(Speech-to-Text), Í∞êÏ†ï Î∂ÑÏÑù, ÎÖ∏Ïù¥Ï¶à ÌïÑÌÑ∞ÎßÅ, ÏùåÏïÖ Ï∂îÏ≤ú ÏãúÏä§ÌÖú

üìå **ÎåÄÌëúÏ†ÅÏù∏ Î™®Îç∏**
- `MFCC (Mel-Frequency Cepstral Coefficients)`: ÏùåÏÑ± Îç∞Ïù¥ÌÑ∞Î•º ÌäπÏßï Î≤°ÌÑ∞Î°ú Î≥ÄÌôò
- `wav2vec 2.0 (by Facebook)`: ÏùåÏÑ±ÏùÑ Î≤°ÌÑ∞ÌôîÌïòÍ≥† ÌÖçÏä§Ìä∏Î°ú Î≥ÄÌôòÌïòÎäî Î™®Îç∏
- `Whisper (by OpenAI)`: Îã§Íµ≠Ïñ¥ ÏùåÏÑ± Ïù∏Ïãù Î∞è ÏûÑÎ≤†Îî© Î™®Îç∏

‚úÖ **ÏòàÏ†ú**: `librosa`Î•º ÏÇ¨Ïö©ÌïòÏó¨ Ïò§ÎîîÏò§ ÏûÑÎ≤†Îî© Î≤°ÌÑ∞ ÏÉùÏÑ±
```python
import librosa

# Ïò§ÎîîÏò§ ÌååÏùº Î°úÎìú
y, sr = librosa.load("speech.wav")

# MFCC ÌäπÏßï Î≤°ÌÑ∞ Ï∂îÏ∂ú
mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)

# Ï∂úÎ†• (Ï≤´ Î≤àÏß∏ ÌîÑÎ†àÏûÑÏùò MFCC Î≤°ÌÑ∞)
print(mfccs[:, 0])
```

---

### 4Ô∏è‚É£ **ÏûÑÎ≤†Îî© Î≤°ÌÑ∞Ïùò ÌôúÏö©**
ÏûÑÎ≤†Îî© Î™®Îç∏ÏùÑ ÌôúÏö©ÌïòÎ©¥ **Î¨∏ÏÑú Í≤ÄÏÉâ, Ï±óÎ¥á, Ï∂îÏ≤ú ÏãúÏä§ÌÖú, RAG Î™®Îç∏** Îì±ÏóêÏÑú Í∞ïÎ†•Ìïú ÏÑ±Îä•ÏùÑ Î∞úÌúòÌï† Ïàò ÏûàÏäµÎãàÎã§.

‚úÖ **Î¨∏ÏÑú Í≤ÄÏÉâ ÏãúÏä§ÌÖú**  
- ÏÇ¨Ïö©ÏûêÍ∞Ä ÏûÖÎ†•Ìïú ÏßàÎ¨∏ÏùÑ Î≤°ÌÑ∞Î°ú Î≥ÄÌôòÌïòÏó¨ **Í∞ÄÏû• Ïú†ÏÇ¨Ìïú Î¨∏ÏÑú**Î•º Í≤ÄÏÉâ  
- Ïòà: **LangChain + ChromaDB** Î•º ÌôúÏö©Ìïú **RAG (Retrieval-Augmented Generation)**  

‚úÖ **Ï±óÎ¥á Î∞è LLM ÏùëÏö©**  
- Ï±óÎ¥áÏù¥ ÎãµÎ≥ÄÏùÑ ÏÉùÏÑ±Ìï† Îïå **ÏûÑÎ≤†Îî© Î≤°ÌÑ∞ Í∏∞Î∞ò Í≤ÄÏÉâ**ÏùÑ ÏàòÌñâ  
- Ïòà: ÏÇ¨Ïö©ÏûêÏùò ÏûÖÎ†•ÏùÑ ÏûÑÎ≤†Îî© ÌõÑ, **Í∞ÄÏû• Í¥ÄÎ†®ÏÑ±Ïù¥ ÎÜíÏùÄ ÎãµÎ≥ÄÏùÑ DBÏóêÏÑú Í≤ÄÏÉâÌïòÏó¨ ÏùëÎãµ**  

‚úÖ **Ï∂îÏ≤ú ÏãúÏä§ÌÖú**  
- Netflix, Spotify Í∞ôÏùÄ Ï∂îÏ≤ú ÏãúÏä§ÌÖúÏóêÏÑú **ÏÇ¨Ïö©ÏûêÏùò Ï∑®Ìñ•ÏùÑ ÏûÑÎ≤†Îî© Î≤°ÌÑ∞Î°ú Î≥ÄÌôò**  
- Ïòà: ÏÇ¨Ïö©ÏûêÍ∞Ä ÏãúÏ≤≠Ìïú ÏòÅÌôî/ÏùåÏïÖÏùò ÏûÑÎ≤†Îî© Î≤°ÌÑ∞Î•º ÌôúÏö©ÌïòÏó¨ **ÎπÑÏä∑Ìïú ÏΩòÌÖêÏ∏† Ï∂îÏ≤ú**  

‚úÖ **ÏùòÎ£å Î∞è ÏÉùÎ¨ºÏ†ïÎ≥¥Ìïô**  
- Ïú†Ï†ÑÏûê Îç∞Ïù¥ÌÑ∞, ÏùòÌïô ÎÖºÎ¨∏, Îã®Î∞±Ïßà Íµ¨Ï°∞ Îç∞Ïù¥ÌÑ∞Î•º Î≤°ÌÑ∞ÌôîÌïòÏó¨ Î∂ÑÏÑù  
- Ïòà: Ïã†ÏïΩ Í∞úÎ∞ú, Ïú†Ï†ÑÏ≤¥ Î∂ÑÏÑù  

---

### 5Ô∏è‚É£ **Í≤∞Î°†**
ÏûÑÎ≤†Îî© Î™®Îç∏ÏùÄ Îç∞Ïù¥ÌÑ∞(ÌÖçÏä§Ìä∏, Ïù¥ÎØ∏ÏßÄ, Ïò§ÎîîÏò§)Î•º **Î≤°ÌÑ∞ ÌëúÌòÑÏúºÎ°ú Î≥ÄÌôòÌïòÏó¨** Ïú†ÏÇ¨ÎèÑ Í≤ÄÏÉâ, Ï±óÎ¥á, Ï∂îÏ≤ú ÏãúÏä§ÌÖú Îì± Îã§ÏñëÌïú Î∂ÑÏïºÏóêÏÑú ÌôúÏö©Îê©ÎãàÎã§.  
ÏµúÍ∑ºÏóêÎäî **Transformer Í∏∞Î∞òÏùò Í∞ïÎ†•Ìïú ÏûÑÎ≤†Îî© Î™®Îç∏(BERT, CLIP, wav2vec 2.0 Îì±)Ïù¥ Îì±Ïû•**ÌïòÎ©¥ÏÑú, **Îçî Ï†ïÍµêÌïú ÏùòÎØ∏ ÌëúÌòÑÏù¥ Í∞ÄÎä•**Ìï¥Ï°åÏäµÎãàÎã§.

> üöÄ **LangChain + Vector DB (Chroma, FAISS) + LLM(RAG)** ÏùÑ ÌôúÏö©ÌïòÎ©¥ ÎçîÏö± Í∞ïÎ†•Ìïú AI ÏùëÏö© ÏãúÏä§ÌÖúÏùÑ Íµ¨Ï∂ïÌï† Ïàò ÏûàÏäµÎãàÎã§!

---

### 3)Llama 3.1 8B Î™®Îç∏ Finetuning (LoRA Ï†ÅÏö©)
Llama 3.1 8B Î™®Îç∏ÏùÑ LoRA (Low-Rank Adaptation) Î∞©ÏãùÏúºÎ°ú ÌååÏù∏ÌäúÎãùÌïòÎ†§Î©¥ PEFT (Parameter-Efficient Fine-Tuning) Î•º ÏÇ¨Ïö©ÌïòÎ©¥ Îê©ÎãàÎã§.

3.1 LoRA ÏÑ§Ïπò

```bash
pip install peft bitsandbytes datasets
```

3.2 LoRA Ï†ÅÏö© ÏΩîÎìú


```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer
from peft import LoraConfig, get_peft_model
from datasets import Dataset

# Î™®Îç∏ Î∞è ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Î°úÎìú
model_name = "../models/Llama-3.1-8b"
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
print(f"Using device: {device}")

# Î™®Îç∏ Î°úÎìú
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,  # ‚úÖ float16 ÎåÄÏã† bfloat16 ÏÇ¨Ïö©
    device_map="auto"
).to(device)  # ‚úÖ Î™ÖÏãúÏ†ÅÏúºÎ°ú ÎîîÎ∞îÏù¥Ïä§Î°ú Ïù¥Îèô

# LoRA ÏÑ§Ï†ï
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.1,
    bias="none",
    target_modules=["q_proj", "v_proj"]  # Llama Î™®Îç∏Ïùò ÌïµÏã¨ Í∞ÄÏ§ëÏπò Ï°∞Ï†ï
)

# LoRA Î™®Îç∏ Ï†ÅÏö©
lora_model = get_peft_model(model, lora_config)

# ‚úÖ ÏûÖÎ†• Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ
train_texts = [
    "Question: Llama Î™®Îç∏ÏùÄ?\nAnswer: MetaÏùò ÎåÄÌòï Ïñ∏Ïñ¥ Î™®Îç∏ÏûÖÎãàÎã§.",
    "Question: RAGÎûÄ?\nAnswer: Í≤ÄÏÉâ Í∏∞Î∞ò ÏÉùÏÑ± Î™®Îç∏ÏûÖÎãàÎã§."
]

# ‚úÖ ÌÜ†ÌÅ∞Ìôî Ìï®Ïàò ÏàòÏ†ï
def tokenize_fn(text):
    tokenized = tokenizer(text, truncation=True, padding="max_length", max_length=64)
    return {
        "input_ids": tokenized["input_ids"],
        "attention_mask": tokenized["attention_mask"],
        "labels": tokenized["input_ids"]  # GPT Î™®Îç∏ÏùÄ labels=input_ids ÏÇ¨Ïö©
    }

# ‚úÖ Îç∞Ïù¥ÌÑ∞ÏÖã Î≥ÄÌôò (Hugging Face Dataset)
train_dataset = Dataset.from_dict({"text": train_texts})
train_dataset = train_dataset.map(lambda x: tokenize_fn(x["text"]), batched=True, remove_columns=["text"])

# ‚úÖ TrainingArguments ÏÑ§Ï†ï (disable removal of unused columns)
training_args = TrainingArguments(
    output_dir="finetuned-llama3",
    per_device_train_batch_size=1,
    num_train_epochs=1,
    save_steps=10,
    logging_steps=5,
    remove_unused_columns=False,  # <--- Added line to fix the error
)

# ‚úÖ Trainer ÏÑ§Ï†ï
trainer = Trainer(
    model=lora_model,
    args=training_args,
    train_dataset=train_dataset,
)

# ‚úÖ ÌïôÏäµ ÏãúÏûë
trainer.train()

# ‚úÖ ÌïôÏäµ ÏôÑÎ£å ÌõÑ Î™®Îç∏ Ï†ÄÏû•
trainer.save_model("finetuned-llama3")

# Save the adapter checkpoint (this will save adapter_config.json, adapter weights, etc.)
lora_model.save_pretrained("finetuned-llama3")
# Optionally, save the tokenizer as well.
tokenizer.save_pretrained("finetuned-llama3")
print("Adapter and tokenizer saved successfully!")


print("Finetuning Complete!")

```

    Using device: mps



    Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]


    'NoneType' object has no attribute 'cadam32bit_grad_fp32'


    /opt/anaconda3/envs/guide/lib/python3.9/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.
      warn("The installed version of bitsandbytes was compiled without GPU support. "



    Map:   0%|          | 0/2 [00:00<?, ? examples/s]




    <div>

      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>
      [2/2 00:00, Epoch 1/1]
    </div>
    <table border="1" class="dataframe">
  <thead>
 <tr style="text-align: left;">
      <th>Step</th>
      <th>Training Loss</th>
    </tr>
  </thead>
  <tbody>
  </tbody>
</table><p>


    Adapter and tokenizer saved successfully!
    Finetuning Complete!


LoRAÎ•º ÌôúÏö©ÌïòÎäî Ïù¥Ïú†:

- ÏùºÎ∞òÏ†ÅÏù∏ Llama 3.1 8B Î™®Îç∏ ÌïôÏäµÏùÄ RAM 100GB Ïù¥ÏÉÅ ÌïÑÏöî
- LoRAÎ•º ÏÇ¨Ïö©ÌïòÎ©¥ ÌäπÏ†ï Í∞ÄÏ§ëÏπò(Q, V ÌîÑÎ°úÏ†ùÏÖò)Îßå Ï°∞Ï†ïÌïòÏó¨ Î©îÎ™®Î¶¨ ÏÇ¨Ïö©ÎüâÏùÑ Ï§ÑÏûÑ
- M4 Pro(64GB RAM) ÌôòÍ≤ΩÏóêÏÑúÎèÑ Ïã§Ìñâ Í∞ÄÎä•

### 4) ÏöîÏïΩ
- Llama 3.1 8B Î™®Îç∏ Ïã§Ìñâ

1. Hugging Face transformers Î°úÎìú
    - mps(Apple Metal Performance Shader) ÏÇ¨Ïö©
    - float16ÏúºÎ°ú Î©îÎ™®Î¶¨ ÏµúÏ†ÅÌôî
    - LangChain + Vector DB Ï†ÅÏö© (RAG)

2. ChromaDBÎ°ú Î¨∏ÏÑú Ï†ÄÏû•
    - LangChainÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ Í≤ÄÏÉâ-ÏùëÎãµ ÏãúÏä§ÌÖú Íµ¨Ï∂ï
    - Llama 3.1 8B Finetuning (LoRA)

3. PEFT(LoRA) ÌôúÏö©ÌïòÏó¨ Í∞ÄÎ≤ºÏö¥ ÌïôÏäµ ÏßÑÌñâ
    - Apple Silicon ÌôòÍ≤ΩÏóêÏÑúÎèÑ Í∞ÄÎä•

### ÌïôÏäµÎêú Î™®Îç∏ ÏÇ¨Ïö© Î∞©Î≤ï


```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

# Load the base model from its original checkpoint
base_model_name = "../models/Llama-3.1-8b"
base_model = AutoModelForCausalLM.from_pretrained(base_model_name)

# Now load the adapter from your local directory
model = PeftModel.from_pretrained(base_model, "finetuned-llama3", local_files_only=True)

# Load the tokenizer (you can load from your adapter folder if saved or from the base model)
tokenizer = AutoTokenizer.from_pretrained("finetuned-llama3")

prompt = "Question: Llama Î™®Îç∏ÏùÄ?\nAnswer:"
inputs = tokenizer(prompt, return_tensors="pt")

with torch.no_grad():
    outputs = model.generate(**inputs, max_new_tokens=50)

generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
print("Generated Response:")
print(generated_text)

```


    Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]


    Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.


    Generated Response:
    Question: Llama Î™®Îç∏ÏùÄ?
    Answer: B


Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Generated Response:
Question: Llama Î™®Îç∏ÏùÄ?
Answer: B


  
Ïûò ÌïôÏäµÎêòÏßÑ ÏïäÏùÄ Î™®ÏäµÏù¥Îã§.
ÏïÑÎßà ÎÑàÎ¨¥ Ï†ÅÏùÄ ÌÅ¨Í∏∞Ïùò Îç∞Ïù¥ÌÑ∞ÏÖãÏùÑ Ïù¥Ïö©Ìï¥ÏÑú Í∑∏Îü∞ Í≤É Í∞ôÎã§
ÌÅ¨Í∏∞Î•º ÎäòÎ†§ÏÑú ÏãúÎèÑÌï¥Î≥¥Î©¥ Ïûò Îê† Í≤É Í∞ôÏäµÎãàÎã§

https://huggingface.co/datasets/SGTCho/korean_food
  
ÏóêÏÑú ÌïúÏãù Í¥ÄÎ†® Îç∞Ïù¥ÌÑ∞ÏÖãÏùÑ Î∞õÏùÄÌõÑ

https://github.com/SGT-Cho/LLM/tree/main/Finetuning
  
Îî∞ÎùºÌï¥Î≥¥Î©¥ Îê† Í≤É Í∞ôÏäµÎãàÎã§


