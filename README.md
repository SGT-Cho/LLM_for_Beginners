# LLM, Llama, Deepseek R1, LangChain, RAG, Vector DB, LLM Finetuning

This notebook has been created to **demonstrate how to experiment with a Local Large Language Model (LLM) in a local environment**.

---

## 1. LLM (Large Language Model)

An **LLM** is a language model with an extremely large number of parameters, showing **high performance** in various areas of **Natural Language Processing (NLP)**.  
- Examples: **GPT-4**, **Llama3**, **Phi4**, **Deepseek R1**, etc.

---

## 2.1 Llama

**Llama** is a **large language model** family released by Meta.  
- Pretrained **Llama models** can be run **locally** via Hugging Face Transformers.

## 2.2 Deepseek R1

**Deepseek R1** is an inference model based on reinforcement learning (RL).  
- DeepSeek is a Chinese AI company established in 2023 by the High-Flyer hedge fund. They have released various open-source large language models such as DeepSeek LLM, DeepSeek Coder, DeepSeek Math, and so on.  
  In particular, the recently released DeepSeek-V3 has gained attention for performance on par with Claude 3.5 Sonnet or Gemini 1.5 Pro.

- Recently, they open-sourced **DeepSeek-R1** and **DeepSeek-R1-Zero**, which maximize inference abilities through **reinforcement learning (RL)**. These two models were open-sourced on January 20, 2025.

---

## 3. LangChain

**LangChain** is a Python-based library that provides a variety of features to **systematically develop** and **scale** applications that utilize **large language models (LLMs)**.

- **Prompt Management**  
  - Organizes and **systematizes** multiple prompts for efficient reuse and management.

- **Chaining**  
  - Allows you to **sequentially connect** multiple steps of LLM tasks in a **pipeline** format.  
  - Example: Summarization â†’ Q&A â†’ Sentiment Analysis, etc. You can easily implement multi-step workflows.

- **Agents**  
  - Modules that can **dynamically perform tasks** by accessing specific tools (API, DB, etc.) and make **reasoning-based** decisions to **solve problems**.

- **Memory**  
  - A feature that manages **previous conversations or context** so that the model can â€œrememberâ€ them.

- **Tool Integration**  
  - Integrates easily with a wide range of external tools (databases, third-party APIs, web search, etc.) to provide **rich functionality**.

Because all of these capabilities can be **combined**, it is easy to **rapidly prototype** LLM-based applications and expand them into **production environments**.

---

## 4. RAG (Retrieval-Augmented Generation)

**RAG (Retrieval-Augmented Generation)** is a method in which a large language model does not rely solely on its **internally learned parameters** but also queries **external knowledge bases** (documents, databases, web search results, etc.) in **real time** to retrieve and use necessary information.

1. **Retrieve**  
   - Uses a search engine such as a **Vector DB** to find **relevant documents or information** corresponding to the user query (or conversation context).  
   - Through similarity search based on **embedding vectors**, the model **quickly obtains** the information it needs.

2. **Generate**  
   - The **LLM** **generates** the answer using the retrieved documents.  
   - Leveraging **specific details** from the documents, the model returns **factually rich** and **highly accurate** text.

**Advantages**  
- **Improved Accuracy**: Can utilize the latest data or knowledge not contained in the modelâ€™s training.  
- **Overcoming Memory Limits**: You do not have to store all knowledge within model parameters, allowing you to **maintain an efficient model size**.  
- **Flexibility**: Can combine various data types (text, image captions, DB content, etc.).

---

## 5. Vector DB

A **Vector DB** is a **specialized database** for storing **embedding vectors** (from text, images, etc.) and performing **fast searches** for vectors (documents, images) that have **high similarity**.

- **Key Features**  
  1. **Vector Insertion**  
     - Converts text/images to vectors using a pretrained model (e.g., SentenceTransformer, BERT, etc.) and stores them.  
  2. **Similarity Search (ANN Search)**  
     - Utilizes Approximate Nearest Neighbor Search techniques to **efficiently** find the **most similar vectors (documents)** among a large vector set.  
  3. **Scalability**  
     - Maintains **fast search speed** even for very large datasets through **scaling** and distributed processing.

- **Representative Vector DB examples**  
  - **FAISS**: A vector similarity search library developed by Meta (formerly Facebook).  
  - **Chroma**: A vector DB that can easily scale from personal projects to enterprise services.  
  - **Milvus**: A high-performance, large-scale vector search engine.  
  - **Pinecone**: A fully managed cloud-based vector DB service.

**Use Cases**  
- **RAG** (Retrieval-Augmented Generation) for **document search**  
- **Similarity-based recommendation** systems  
- **Multi-modal** search for images, audio, etc.

---

## 6. LLM Finetuning

**LLM Finetuning** is the process of **additional training** a pretrained large language model (e.g., GPT, BERT, etc.) to **customize** it for a specific task or domain.  
- You can **significantly improve** the modelâ€™s performance on **specific data** while reusing existing parameters.

### 6.1 Finetuning Methods

1. **Full Finetuning**  
   - Updating **all** model parameters, usually through **epoch-based** optimization.  
   - Used when you have plenty of data and computational resources.

2. **PEFT (LoRA, Prefix Tuning, etc.)**  
   - Approaches that **update only part of the parameters** or that enable **low-cost** additional training.  
   - **LoRA (Low-Rank Adaptation)**: Trains certain weight matrices in the model in a **low-rank** form to achieve finetuning effects with **less memory usage**.  
   - **Prefix Tuning**: Adds a **virtual prompt (prefix)** before the input tokens, guiding model performance improvement **without major changes** to the main model.

3. **Training Tools**  
   - **Hugging Face Transformers** libraryâ€™s **Trainer API**  
   - **Deepspeed**, **Accelerate** for **distributed training** and memory optimization  
   - **PEFT library**: Apply LoRA and various other techniques easily

### 6.2 Considerations

- **Data Quality**: The **domain suitability** and **label quality** of the finetuning data are very important.  
- **Avoid Overfitting**: Simply increasing the learning rate or the number of epochs may cause the **modelâ€™s generative ability to degrade** or lead to **model bias**.  
- **Model Compatibility**: Some models have architectural constraints that limit finetuning, so you should consult **official documentation** or **community resources**.

---

By combining **LangChain**, **RAG**, **Vector DB**, and **LLM Finetuning**, you can:

1. Easily build an **LLM-based pipeline**,  
2. Strengthen the modelâ€™s responses with **accurate and rich knowledge**, and  
3. Create a **custom LLM** optimized for a **specific domain** or **work environment**.

In practical or research settings where LLMs are utilized:  
- Use **LangChain** to structure your **workflow**,  
- Enhance it with **RAG** for **real-time knowledge retrieval**,  
- Maximize **search performance** with a **Vector DB**,  
- And **LLM Finetuning** to adapt the model to **domain-specific** needs.  

This will enable you to build more **efficient** and **powerful** NLP solutions.

---

## Notebook Demonstration

In this notebook, we will explore the following **simple example code** to see how to use a local environment:

1. **Load a local Llama model (or similar) and perform simple inference**  
2. **Use LangChain + Vector DB to demonstrate RAG**  
3. (Simple version) **LLM Finetuning example**

```bash
pip install -r requirements.txt
```
Run this in your terminal to set up the environment.

Example: Loading a local LLM
Below is an example of downloading the Llama3.1-8b model from the Hugging Face model repository to your local machine and performing a simple inference.

On Apple Silicon (M1, M4, etc.), mps (Metal Performance Shaders) device support may be set automatically or require manual configuration.

â¸»

Tokens and Tokenizer

In Natural Language Processing (NLP), tokenization is a very important preprocessing step that breaks text into specific units. The tool or library used in this process is the tokenizer. Letâ€™s learn about this in more detail.
1.	What is a Token?

	A token is a small semantic unit obtained by splitting text (sentences, paragraphs, etc.). The way you split can vary, producing different types of token units.
	
	1.1 Word-level
		â€¢A simple method that splits based on whitespace or punctuation.
		â€¢Example: â€œë‚˜ëŠ” í•™êµì— ê°„ë‹¤.â€ â†’ [â€œë‚˜ëŠ”â€, â€œí•™êµì—â€, â€œê°„ë‹¤.â€]
	
	1.2 Morpheme-level (used in Korean)
		â€¢Often used for Korean.
		â€¢Example: â€œë‚˜ëŠ” í•™êµì— ê°„ë‹¤.â€ â†’ [â€œë‚˜â€, â€œëŠ”â€, â€œí•™êµâ€, â€œì—â€, â€œê°€â€, â€œá„‚ë‹¤â€, â€œ.â€]
	
	1.3 Subword-level
		â€¢Uses algorithms like BPE, WordPiece, SentencePiece to split into smaller-than-word units.
		â€¢Example: â€œunhappyâ€ â†’ [â€œunâ€, â€œhappyâ€]
		â€¢Example: â€œunbelievableâ€ â†’ [â€œunâ€, â€œbelievableâ€]

2.	What is a Tokenizer?

	A tokenizer is a tool (or library) that splits text into tokens according to certain rules or algorithms.
	
	2.1 Rule-based Tokenizer
		â€¢Splits using predefined rules such as whitespace, punctuation, or patterns (regex).
		â€¢Example: split(), simple regex-based splitting.
	
	2.2 Trained Tokenizer
		â€¢Automatically creates a token vocabulary by training on a corpus of data, learning the rules.
		â€¢BPE (Byte-Pair Encoding), WordPiece, SentencePiece are representative examples.
		â€¢Also used in large language models (e.g., BERT, GPT).

3.	Why is Tokenization Important?
4.	Improved Accuracy
	â€¢Splitting text into correct units increases the accuracy of subsequent tasks like morphological analysis or POS tagging.
5.	Vocabulary Management
	â€¢If you only split by words, your vocabulary can become huge, but with subword methods, you can handle rare words and neologisms efficiently.
6.	Maximize Model Performance
	â€¢Poor tokenization can make it difficult to train the model or degrade performance at inference time.
	â€¢Models like BERT or GPT are trained on consistent tokenization rules.
7.	Considerations for Korean Tokenization
8.	The need for morphological analysis
	â€¢Korean has various postpositions and endings, making it difficult to tokenize by whitespace alone.
9.	Various elements within a word
	â€¢â€œí•™êµì—â€ â†’ [â€œí•™êµâ€, â€œì—â€]
	â€¢â€œí•™êµì—ì„œâ€ â†’ [â€œí•™êµâ€, â€œì—ì„œâ€]
	â€¢Must split postpositions, endings, etc. to get the desired accuracy.
10.	Irregular spacing
	â€¢Because spacing is often not strictly followed, purely rule-based approaches have limitations.
11.	Representative Tokenizers

	â€¢NLTK (English)
	â€¢One of the most widely used NLP libraries in Python
	â€¢Provides word tokenization, sentence tokenization, stopword removal, etc.
	â€¢KoNLPy (Korean)
	â€¢Integrates multiple morphological analyzers (Twitter, Kkma, Hannanum, etc.)
	â€¢Specialized for Korean, offering morphological analysis, POS tagging, etc.
	â€¢BPE, SentencePiece, WordPiece
	â€¢Subword-based tokenizers
	â€¢Used in large language models such as BERT, GPT, RoBERTa, etc.

6.	Summary

	â€¢Token
	â€¢The result of splitting text into small units (words, morphemes, subwords, etc.)
	â€¢Tokenizer
	â€¢A tool or algorithm for splitting text according to a specified criterion
	â€¢Tokenization
	â€¢A core preprocessing step in NLP. Good tokenization can significantly improve model performance and manage vocabulary effectively.
	â€¢For Korean, morphological analysis or subword-based methods are commonly used.

In conclusion, proper tokenization can greatly improve data quality and model performance. It is important to choose a tokenizer suited to your NLP project and devise a tokenization strategy that reflects the linguistic characteristics of each language.

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# Example for a Llama model (In practice, you may need permission or a different model name in Hugging Face.)
# model_name = "meta-llama/Llama-3.1-8B"
model_name="../models/Llama-3.1-8b"  
# Check if the mps device is available
device = torch.device("mps") if torch.backends.mps.is_available() else torch.device("cpu")
print("Using device:", device)

# Load tokenizer and model
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map={"": device})

# Simple text generation example
prompt = "Hello, how are you?"
inputs = tokenizer(prompt, return_tensors="pt").to(device)
with torch.no_grad():
    outputs = model.generate(**inputs, max_new_tokens=50)
generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
print("Generated text:")
print(generated_text)

Using device: mps
...
Generated text:
Hello, how are you? Today we are going to talk about the new generation...

Explanation:
	â€¢torch_dtype=torch.float16: float16 is optimal on Apple MPS.
	â€¢device_map={"": device}: Automatically places the model on MPS (GPU) or CPU.
	â€¢max_new_tokens=50: Limits response to 50 tokens.

Llama 3.1 8B + LangChain + Vector DB (RAG)

You can also integrate the Llama 3.1 8B model with LangChain and ChromaDB (a Vector DB) to perform Retrieval-Augmented Generation (RAG).

2.1 Install LangChain & ChromaDB

pip install langchain chromadb faiss-cpu sentence-transformers

2.2 RAG Code Example

# Required library imports
from transformers import pipeline  # <-- added here
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.docstore.document import Document
from langchain.chains import RetrievalQA
from langchain.llms import HuggingFacePipeline

# 1) Load embedding model
embedding_model = "sentence-transformers/all-MiniLM-L6-v2"
embeddings = HuggingFaceEmbeddings(model_name=embedding_model)

# Sample documents
texts = [
    "LangChain is a framework for chaining LLMs.",
    "RAG stands for Retrieval-Augmented Generation.",
    "A Vector DB is a database for searching document embedding vectors."
]
documents = [Document(page_content=t) for t in texts]

# 2) Initialize a Chroma VectorStore
vectorstore = Chroma.from_documents(documents, embedding=embeddings, collection_name="example_collection")

# 3) Connect Llama 3.1 8B model to LangChain
generator_pipeline = pipeline(
    "text-generation",
    model=model,  # Llama 3.1 8B model
    tokenizer=tokenizer
)
llm = HuggingFacePipeline(pipeline=generator_pipeline)

# 4) Create a RetrievalQA chain
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=vectorstore.as_retriever()
)

# 5) Ask a question and generate RAG-based answer
query = "What is RAG?"
answer = qa_chain.run(query)
print(f"Q: {query}\nA: {answer}")

Output:

Q: What is RAG?
A: Retrieval-Augmented Generation...



â¸»

What is an Embedding Model?

1) Concept

An Embedding Model is a model that converts data into fixed-size vectors.
It transforms various types of data (text, images, audio, etc.) into numerical vectors so that computers can understand and process them.

In other words, embedding is the process of converting high-dimensional data (words, sentences, documents, images, etc.) into dense vectors, and the model that generates these vectors is called an Embedding Model.

â¸»

2) Why are embeddings needed?

Because computers can only understand numbers, they cannot directly process natural language (NLP) or images.
Hence, Embedding Models are needed for the following reasons:
	â€¢	Convert strings to numbers: Text must be converted into numerical vectors for machine learning models to understand.
	â€¢	Similar data have similar vector values: Words with similar meanings end up with similar vector representations.
	â€¢	Compress high-dimensional data into lower dimensions: Embeddings help reduce the dimensionality for more optimized computations.

For example, â€œcatâ€ and â€œdogâ€ are similar in meaning, so their embedding vectors are close to each other in the vector space.
Meanwhile, â€œcatâ€ and â€œcarâ€ have little connection, so they lie far apart in the vector space.

â¸»

3) Types of Embedding Models

Embedding Models are used to vectorize text, images, audio, etc. Here are some representative types:

(1) Text Embeddings (Word/Sentence Embeddings)
	â€¢Models that convert words, sentences, or documents into vectors for NLP
	â€¢Uses: Chatbots, search, recommendation systems, document classification, RAG

Representative Models:
	â€¢Word2Vec: A leading model for word vectorization
	â€¢GloVe: Vectors that reflect statistical co-occurrence of words
	â€¢FastText: An improvement on Word2Vec (can embed subwords)
	â€¢BERT Embedding: A powerful model that considers context when creating vectors
	â€¢Sentence-BERT (SBERT): A model that creates embeddings at the sentence level
	â€¢sentence-transformers/all-MiniLM-L6-v2: A lightweight, fast sentence embedding model (often used with LangChain, RAG)

Example (using sentence-transformers to generate text embedding vectors):

from sentence_transformers import SentenceTransformer

# Load an SBERT-based sentence embedding model
model = SentenceTransformer("all-MiniLM-L6-v2")

# Example sentences
sentences = ["Cats are cute.", "Dogs are loyal.", "Cars are fast."]

# Convert to embeddings (vectors)
embeddings = model.encode(sentences)

# Print the vector for the first sentence
print(embeddings[0])



â¸»

(2) Image Embeddings
	â€¢Convert image data into vectors for tasks like image similarity search, object recognition, etc.
	â€¢Uses: Image retrieval systems, style recommendations, computer vision

Representative Models:
	â€¢ResNet-50, EfficientNet, CLIP: For image classification and feature extraction
	â€¢DINOv2: An image embedding model recently introduced by Meta
	â€¢OpenAI CLIP: Maps text and images into the same vector space (e.g. â€œdog photoâ€ â†’ close to dog-image vectors)

Example (using CLIP to generate image embedding vectors):

import torch
import clip
from PIL import Image

# Load CLIP model
device = "cuda" if torch.cuda.is_available() else "cpu"
model, preprocess = clip.load("ViT-B/32", device=device)

# Load and transform the image
image = preprocess(Image.open("dog.jpg")).unsqueeze(0).to(device)

# Extract embedding vectors
with torch.no_grad():
    image_features = model.encode_image(image)

print(image_features.shape)  # e.g., [1, 512]



â¸»

(3) Audio Embeddings
	â€¢Convert audio data into vectors for tasks like speech recognition, emotion analysis, etc.
	â€¢Uses: Speech-to-text, emotion analysis, noise filtering, music recommendation systems

Representative Models:
	â€¢MFCC (Mel-Frequency Cepstral Coefficients): A method to extract feature vectors from audio signals
	â€¢wav2vec 2.0 (by Facebook): Converts audio to vectors and can transcribe text
	â€¢Whisper (by OpenAI): A multi-language speech recognition and embedding model

Example (using librosa to generate audio embedding vectors):

import librosa

# Load an audio file
y, sr = librosa.load("speech.wav")

# Extract MFCC feature vectors
mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)

# Print the MFCC vector for the first frame
print(mfccs[:, 0])



â¸»

4) Applications of Embedding Vectors

Using embedding models, you can achieve strong performance in tasks such as document search, chatbots, recommendation systems, and RAG models.
	â€¢Document Retrieval System
	â€¢Convert user queries into vectors and search for the most similar documents.
	â€¢Example: LangChain + ChromaDB for RAG (Retrieval-Augmented Generation)
	â€¢Chatbots and LLM Applications
	â€¢During chatbot response generation, utilize embedding-based searches.
	â€¢Example: Convert user input into embeddings and retrieve the most relevant answers from a DB.
	â€¢Recommendation Systems
	â€¢Platforms like Netflix or Spotify use embedding vectors for user preference and content.
	â€¢Example: Embedding vectors for movies/music the user has consumed, then recommend similar content.
	â€¢Healthcare and Bioinformatics
	â€¢Vectorize gene data, medical publications, protein structures, etc. for analysis.
	â€¢Example: Drug discovery, genome analysis

â¸»

5) Conclusion

Embedding models convert text, images, audio into vector representations, enabling similarity search, chatbots, recommendation systems, and more.
Transformer-based embedding models (e.g., BERT, CLIP, wav2vec 2.0, etc.) are becoming more sophisticated, enabling richer semantic representations.

ğŸš€ By combining LangChain + Vector DB (Chroma, FAISS) + LLM (RAG), you can build even more powerful AI applications!

â¸»

3) Llama 3.1 8B Model Finetuning (LoRA Application)

To finetune the Llama 3.1 8B model using LoRA (Low-Rank Adaptation), you can use PEFT (Parameter-Efficient Fine-Tuning).

3.1 Install LoRA

pip install peft bitsandbytes datasets

3.2 LoRA Code Example

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer
from peft import LoraConfig, get_peft_model
from datasets import Dataset

# Load model and tokenizer
model_name = "../models/Llama-3.1-8b"
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
print(f"Using device: {device}")

# Load the model
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,  # âœ… using bfloat16 instead of float16
    device_map="auto"
).to(device)  # explicitly move model to device

# LoRA Configuration
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.1,
    bias="none",
    target_modules=["q_proj", "v_proj"]  # Adjust core weights of Llama
)

# Apply LoRA to the model
lora_model = get_peft_model(model, lora_config)

# âœ… Prepare input data
train_texts = [
    "Question: What is the Llama model?\nAnswer: It is a large language model by Meta.",
    "Question: What is RAG?\nAnswer: It is a retrieval-based generation model."
]

# âœ… Modified tokenization function
def tokenize_fn(text):
    tokenized = tokenizer(text, truncation=True, padding="max_length", max_length=64)
    return {
        "input_ids": tokenized["input_ids"],
        "attention_mask": tokenized["attention_mask"],
        "labels": tokenized["input_ids"]  # GPT-style models use labels = input_ids
    }

# âœ… Convert to Hugging Face Dataset
train_dataset = Dataset.from_dict({"text": train_texts})
train_dataset = train_dataset.map(lambda x: tokenize_fn(x["text"]), batched=True, remove_columns=["text"])

# âœ… TrainingArguments (disable removal of unused columns)
training_args = TrainingArguments(
    output_dir="finetuned-llama3",
    per_device_train_batch_size=1,
    num_train_epochs=1,
    save_steps=10,
    logging_steps=5,
    remove_unused_columns=False,  # <--- Important
)

# âœ… Trainer setup
trainer = Trainer(
    model=lora_model,
    args=training_args,
    train_dataset=train_dataset,
)

# âœ… Start training
trainer.train()

# âœ… Save the trained model
trainer.save_model("finetuned-llama3")

# Save the LoRA adapter checkpoint
lora_model.save_pretrained("finetuned-llama3")
# Optionally save the tokenizer too
tokenizer.save_pretrained("finetuned-llama3")
print("Adapter and tokenizer saved successfully!")

print("Finetuning Complete!")

Using device: mps
...
Finetuning Complete!

Why LoRA?
	â€¢Typical training for Llama 3.1 8B might require over 100GB of RAM.
	â€¢By adjusting only certain weights (Q, V projections) using LoRA, you can reduce memory usage.
	â€¢Can run even on an M4 Pro with 64GB RAM.

4) Summary
	â€¢Llama 3.1 8B model usage
	1.Use Hugging Face Transformers
		â€¢Use mps (Apple Metal)
		â€¢Use float16 for memory optimization
		â€¢Integrate with LangChain + Vector DB (RAG)
	2.Use ChromaDB to store documents
		â€¢Build retrieval-answer systems with LangChain
		â€¢Finetune Llama 3.1 8B (LoRA)
	3.Apply PEFT (LoRA) for lightweight training
		â€¢Possible on Apple Silicon

How to Use the Trained Model

from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

# Load the base model from its original checkpoint
base_model_name = "../models/Llama-3.1-8b"
base_model = AutoModelForCausalLM.from_pretrained(base_model_name)

# Now load the adapter from your local directory
model = PeftModel.from_pretrained(base_model, "finetuned-llama3", local_files_only=True)

# Load the tokenizer (either from your adapter folder or the base model)
tokenizer = AutoTokenizer.from_pretrained("finetuned-llama3")

prompt = "Question: What is the Llama model?\nAnswer:"
inputs = tokenizer(prompt, return_tensors="pt")

with torch.no_grad():
    outputs = model.generate(**inputs, max_new_tokens=50)

generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
print("Generated Response:")
print(generated_text)

Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Generated Response:
Question: What is the Llama model?
Answer: B

It seems the model did not learn wellâ€”likely because the training dataset was very small. If you increase the dataset size, results will likely improve.

For example, you could try a Korean food dataset from:
https://huggingface.co/datasets/SGTCho/korean_food

And follow the steps here:
https://github.com/SGT-Cho/LLM/tree/main/Finetuning




# LLM, Llama, Deepseek R1, LangChain, RAG, Vector DB, LLM Finetuning

ë³¸ ë…¸íŠ¸ë¶ì€ **ë¡œì»¬ í™˜ê²½ì—ì„œ LLM(Local Large Language Model)ì„ í™œìš©í•˜ì—¬ ì‹¤í—˜**í•˜ëŠ” ë°©ë²•ì„ ë°ëª¨í•˜ê¸° ìœ„í•´ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤.  

---

## 1. LLM (Large Language Model)

**LLM**ì€ ë°©ëŒ€í•œ íŒŒë¼ë¯¸í„°ë¥¼ ê°–ì¶˜ ì–¸ì–´ ëª¨ë¸ë¡œ, **ìì—°ì–´ ì²˜ë¦¬(NLP)** ì „ë°˜ì— ê±¸ì³ **ë†’ì€ ì„±ëŠ¥**ì„ ë³´ì…ë‹ˆë‹¤.  
- ì˜ˆ: **GPT-4**, **Llama3**, **Phi4**, **Deepseek R1** ë“±

---

## 2.1 Llama

**Llama**ëŠ” Metaì—ì„œ ê³µê°œí•œ **ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸** ê³„ì—´ì…ë‹ˆë‹¤.  
- ì‚¬ì „ì— í•™ìŠµëœ **Llama ëª¨ë¸**ì„ Hugging Face Transformersë¥¼ í†µí•´ **ë¡œì»¬ì—ì„œ ì‹¤í–‰** ê°€ëŠ¥í•©ë‹ˆë‹¤.
  
## 2.2 Deepseek R1  
  
**Deepseek R1**ì€ ê°•í™” í•™ìŠµ ê¸°ë°˜ ì¶”ë¡  ëª¨ë¸ì…ë‹ˆë‹¤.
- DeepSeek ëŠ” ì¤‘êµ­ í—¤ì§€ í€ë“œì¸ í•˜ì´-í”Œë¼ì´ì–´ (High-Flyer) ê°€ 2023ë…„ì— ì„¤ë¦½í•œ ì¤‘êµ­ì˜ ì¸ê³µì§€ëŠ¥ íšŒì‚¬ì…ë‹ˆë‹¤. DeepSeek LLM, DeepSeek Coder, DeepSeek Math ë“± ë‹¤ì–‘í•œ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì„ ì˜¤í”ˆì†ŒìŠ¤ë¡œ ê³µê°œí•´ ì™”ìŠµë‹ˆë‹¤.  
íŠ¹íˆ, ìµœê·¼ ê³µê°œëœ DeepSeek-V3ëŠ” Claude 3.5 Sonnet ë° Gemini 1.5 Proì™€ ê²¬ì¤„ ë§Œí•œ ì„±ëŠ¥ìœ¼ë¡œ ì£¼ëª©ë°›ê³  ìˆìŠµë‹ˆë‹¤.  

- ì´ë²ˆì—ëŠ” ê°•í™” í•™ìŠµ (RL, Reinforcement Learning) ì„ í†µí•´ ì¶”ë¡  ëŠ¥ë ¥ì„ ê·¹ëŒ€í™”í•œ ìƒˆë¡œìš´ ëª¨ë¸, DeepSeek-R1 ê³¼ DeepSeek-R1-Zero ë¥¼ ê³µê°œí•˜ì˜€ìŠµë‹ˆë‹¤. ì´ ë‘ ëª¨ë¸ì€ 2025ë…„ 1ì›” 20ì¼ì— ì˜¤í”ˆì†ŒìŠ¤ë¡œ ê³µê°œë˜ì—ˆìŠµë‹ˆë‹¤.
---

## 3. LangChain

**LangChain**ì€ íŒŒì´ì¬ ê¸°ë°˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ, **ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(LLM)ì„ í™œìš©í•œ ì• í”Œë¦¬ì¼€ì´ì…˜**ì„ **ì²´ê³„ì ìœ¼ë¡œ ê°œë°œ**í•˜ê³  **í™•ì¥**í•˜ê¸° ìœ„í•œ ë‹¤ì–‘í•œ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.

- **Prompt ê´€ë¦¬**  
  - ì—¬ëŸ¬ í”„ë¡¬í”„íŠ¸ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ê´€ë¦¬í•˜ê³  ì¬ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ **ì²´ê³„í™”**í•©ë‹ˆë‹¤.

- **ì²´ì´ë‹(Chaining)**  
  - ì—¬ëŸ¬ ìŠ¤í…ì˜ LLM ì‘ì—…ì„ **ìˆœì°¨ì ìœ¼ë¡œ ì—°ê²°**í•´ **íŒŒì´í”„ë¼ì¸** í˜•íƒœë¡œ êµ¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.  
  - ì˜ˆ) ìš”ì•½ â†’ ì§ˆì˜ì‘ë‹µ â†’ ê°ì„± ë¶„ì„ ë“±, ë‹¤ë‹¨ê³„ ì›Œí¬í”Œë¡œìš°ë¥¼ ê°„ë‹¨íˆ êµ¬í˜„

- **ì—ì´ì „íŠ¸(Agent)**  
  - íŠ¹ì • íˆ´(API, DB ë“±)ì— ì ‘ê·¼í•´ **ë™ì ìœ¼ë¡œ ì‘ì—…**ì„ ìˆ˜í–‰í•˜ê³ , **ì¶”ë¡ (Reasoning)** ê¸°ë°˜ ì˜ì‚¬ê²°ì •ì„ ë‚´ë ¤ **ë¬¸ì œ í•´ê²°**ì„ ë•ëŠ” ëª¨ë“ˆ

- **ë©”ëª¨ë¦¬(Memory)**  
  - ëª¨ë¸ì´ **ì´ì „ ëŒ€í™”ë‚˜ ì»¨í…ìŠ¤íŠ¸**ë¥¼ **ê¸°ì–µ**í•  ìˆ˜ ìˆë„ë¡ ê´€ë¦¬í•˜ëŠ” ê¸°ëŠ¥

- **íˆ´ ì—°ë™**  
  - ë°ì´í„°ë² ì´ìŠ¤, ì„œë“œíŒŒí‹° API, ì›¹ ê²€ìƒ‰ ë“± ë‹¤ì–‘í•œ **ì™¸ë¶€ íˆ´**ê³¼ ì†ì‰½ê²Œ ì—°ë™í•´ **í’ë¶€í•œ ê¸°ëŠ¥**ì„ ì œê³µ

ì´ ëª¨ë“  ê¸°ëŠ¥ì„ **í†µí•©ì ìœ¼ë¡œ** ì‚¬ìš©í•  ìˆ˜ ìˆì–´, **LLMì„ í™œìš©í•œ ì• í”Œë¦¬ì¼€ì´ì…˜**ì„ ë¹ ë¥´ê²Œ **í”„ë¡œí† íƒ€ì´í•‘**í•˜ê³ , **ìƒì‚° í™˜ê²½**ìœ¼ë¡œ í™•ì¥í•˜ê¸° ì‰½ìŠµë‹ˆë‹¤.

---

## 4. RAG (Retrieval-Augmented Generation)

**RAG(Retrieval-Augmented Generation)**ëŠ” ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸ì´ ê¸°ì¡´ì— í•™ìŠµëœ **íŒŒë¼ë¯¸í„°**ì—ë§Œ ì˜ì¡´í•˜ì§€ ì•Šê³ , ëª¨ë¸ **ì™¸ë¶€**ì˜ ì§€ì‹ ë² ì´ìŠ¤(ë¬¸ì„œ, ë°ì´í„°ë² ì´ìŠ¤, ì›¹ ê²€ìƒ‰ ê²°ê³¼ ë“±)ì—ì„œ **ì‹¤ì‹œê°„**ìœ¼ë¡œ ì •ë³´ë¥¼ ê²€ìƒ‰í•´ í™œìš©í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.

1. **Retrieve (ê²€ìƒ‰)**  
   - ì‚¬ìš©ìì˜ ì§ˆì˜(ë˜ëŠ” ëŒ€í™” ë§¥ë½)ì— ëŒ€ì‘ë˜ëŠ” **ê´€ë ¨ ë¬¸ì„œë‚˜ ì •ë³´**ë¥¼ ì°¾ê¸° ìœ„í•´, **Vector DB** ë“±ì˜ ê²€ìƒ‰ ì—”ì§„ì„ ì‚¬ìš©  
   - **ì„ë² ë”© ë²¡í„°** ê¸°ë°˜ì˜ ìœ ì‚¬ë„ ê²€ìƒ‰ì„ í†µí•´, ëª¨ë¸ì´ í•„ìš”í•œ ì •ë³´ë¥¼ **ë¹ ë¥´ê²Œ íšë“**

2. **Generate (ìƒì„±)**  
   - ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ í† ëŒ€ë¡œ **LLM**ì´ **ì‘ë‹µ**ì„ ìƒì„±  
   - ëª¨ë¸ì€ **ë¬¸ì„œì˜ êµ¬ì²´ì ì¸ ë‚´ìš©**ì„ ë°”íƒ•ìœ¼ë¡œ, **ì‚¬ì‹¤ì ìœ¼ë¡œ í’ë¶€**í•˜ê³  **ì •í™•ë„ ë†’ì€** í…ìŠ¤íŠ¸ë¥¼ ë°˜í™˜

**ì¥ì **  
- **ì •í™•ë„ í–¥ìƒ**: ìµœì‹  ì •ë³´ë‚˜ ëª¨ë¸ì´ í•™ìŠµí•˜ì§€ ëª»í•œ ì§€ì‹ì„ í™œìš© ê°€ëŠ¥  
- **ë©”ëª¨ë¦¬ ì œí•œ ê·¹ë³µ**: ëª¨ë“  ì§€ì‹ì„ ëª¨ë¸ íŒŒë¼ë¯¸í„°ì— ë‹´ì§€ ì•Šì•„ë„ ë˜ë¯€ë¡œ, ëª¨ë¸ í¬ê¸°ë¥¼ **íš¨ìœ¨ì ìœ¼ë¡œ ìœ ì§€**í•  ìˆ˜ ìˆìŒ  
- **ìœ ì—°ì„±**: ë‹¤ì–‘í•œ í˜•íƒœì˜ ë°ì´í„°(í…ìŠ¤íŠ¸, ì´ë¯¸ì§€ ì„¤ëª…, DB ë‚´ìš© ë“±)ì™€ ê²°í•© ê°€ëŠ¥

---

## 5. Vector DB

**Vector DB**ëŠ” í…ìŠ¤íŠ¸ë‚˜ ì´ë¯¸ì§€ë¥¼ **ì„ë² ë”© ë²¡í„°**ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥í•˜ê³ , ì´ì™€ **ìœ ì‚¬ë„ê°€ ë†’ì€** ë²¡í„°(ë¬¸ì„œ, ì´ë¯¸ì§€ ë“±)ë¥¼ **ë¹ ë¥´ê²Œ ê²€ìƒ‰**í•˜ê¸° ìœ„í•œ **íŠ¹í™”ëœ ë°ì´í„°ë² ì´ìŠ¤**ì…ë‹ˆë‹¤.

- **ì£¼ìš” ê¸°ëŠ¥**  
  1. **ë²¡í„° ì‚½ì…**  
     - ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸(ì˜ˆ: SentenceTransformer, BERT ë“±)ë¡œ **í…ìŠ¤íŠ¸/ì´ë¯¸ì§€ â†’ ë²¡í„°**ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥  
  2. **ìœ ì‚¬ë„ ê²€ìƒ‰(ANN Search)**  
     - Approximate Nearest Neighbor Search ê¸°ë²•ì„ í™œìš©í•´, ëŒ€ê·œëª¨ ë²¡í„° ì§‘í•© ë‚´ì—ì„œ **ë¹„ìŠ·í•œ ë²¡í„°(ë¬¸ì„œ)**ë¥¼ **íš¨ìœ¨ì ìœ¼ë¡œ** ì°¾ìŒ  
  3. **í™•ì¥ì„±**  
     - ë°ì´í„°ê°€ ë§¤ìš° ë§ì•„ë„ **í™•ì¥(Scaling)**ê³¼ ë¶„ì‚° ì²˜ë¦¬ë¥¼ í†µí•´ **ë¹ ë¥¸ ê²€ìƒ‰ ì†ë„**ë¥¼ ìœ ì§€

- **ëŒ€í‘œì ì¸ Vector DB ì˜ˆì‹œ**  
  - **FAISS**: ë©”íƒ€(êµ¬ í˜ì´ìŠ¤ë¶)ì—ì„œ ê°œë°œí•œ ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰ ë¼ì´ë¸ŒëŸ¬ë¦¬  
  - **Chroma**: ê°œì¸ í”„ë¡œì íŠ¸ë¶€í„° ëŒ€ê·œëª¨ ì„œë¹„ìŠ¤ê¹Œì§€ ì‰½ê²Œ í™•ì¥ ê°€ëŠ¥í•œ ë²¡í„° DB  
  - **Milvus**: ê³ ì„±ëŠ¥, ëŒ€ê·œëª¨ ë²¡í„° ê²€ìƒ‰ ì—”ì§„  
  - **Pinecone**: í´ë¼ìš°ë“œ ê¸°ë°˜ì˜ ì™„ì „ê´€ë¦¬í˜• ë²¡í„° DB ì„œë¹„ìŠ¤

**í™œìš© ì‚¬ë¡€**  
- **RAG**(Retrieval-Augmented Generation)ì—ì„œ **ë¬¸ì„œ ê²€ìƒ‰**  
- **ìœ ì‚¬ë„ ê¸°ë°˜ ì¶”ì²œ** ì‹œìŠ¤í…œ  
- ì´ë¯¸ì§€ ê²€ìƒ‰, ìŒì„± ê²€ìƒ‰ ë“± **ë©€í‹°ëª¨ë‹¬ ê²€ìƒ‰**

---

## 6. LLM Finetuning

**LLM Finetuning**ì€ ì‚¬ì „ í•™ìŠµëœ ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(ì˜ˆ: GPT, BERT ë“±)ì„ íŠ¹ì • íƒœìŠ¤í¬ë‚˜ ë„ë©”ì¸ì— **ë§ì¶¤í™”**í•˜ê¸° ìœ„í•´ ì¶”ê°€ í•™ìŠµí•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.  
- ê¸°ì¡´ íŒŒë¼ë¯¸í„°ë¥¼ ì¬í™œìš©í•˜ë©´ì„œ, **íŠ¹ì • ë°ì´í„°**ì— ëŒ€í•œ ëª¨ë¸ ì„±ëŠ¥ì„ **í¬ê²Œ ê°œì„ **í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### 6.1 íŒŒì¸íŠœë‹ ë°©ë²•

1. **ì „ì²´ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸(Full Finetuning)**  
   - ëª¨ë¸ì˜ ëª¨ë“  íŒŒë¼ë¯¸í„°ë¥¼ ëŒ€ìƒìœ¼ë¡œ, **ì—í¬í¬(epoch)** ë‹¨ìœ„ë¡œ **ìµœì í™”(Optimization)** ì§„í–‰  
   - ë°ì´í„°ì™€ ê³„ì‚° ìì›ì´ í’ë¶€í•œ ê²½ìš°ì— ì‚¬ìš©

2. **PEFT(LoRA, Prefix Tuning ë“±)**  
   - **ë¶€ë¶„ íŒŒë¼ë¯¸í„°ë§Œ ì—…ë°ì´íŠ¸**í•˜ê±°ë‚˜, **ì €ë¹„ìš©**ìœ¼ë¡œ ì¶”ê°€ í•™ìŠµí•˜ëŠ” ë°©ì‹  
   - **LoRA(Low-Rank Adaptation)**: ëª¨ë¸ ë‚´ë¶€ì˜ íŠ¹ì • ê°€ì¤‘ì¹˜ í–‰ë ¬ì„ **ì €ë­í¬(ì¶•ì†Œëœ ì°¨ì›)** í˜•íƒœë¡œ í•™ìŠµí•´, **ì ì€ ë©”ëª¨ë¦¬**ë¡œë„ íŒŒì¸íŠœë‹ íš¨ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŒ  
   - **Prefix Tuning**: ì…ë ¥ í† í° ì•ì— **ê°€ìƒ í”„ë¡¬í”„íŠ¸(prefix)**ë¥¼ ì¶”ê°€ í•™ìŠµí•´, **ëª¨ë¸ ë³¸ì²´ëŠ” í¬ê²Œ ê±´ë“œë¦¬ì§€ ì•Šê³ ** ì„±ëŠ¥ í–¥ìƒì„ ìœ ë„

3. **í›ˆë ¨ ë„êµ¬**  
   - **Hugging Face Transformers** ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ **Trainer API**  
   - **Deepspeed**, **Accelerate** ë“± **ë¶„ì‚° í›ˆë ¨**, ë©”ëª¨ë¦¬ ìµœì í™” ë¼ì´ë¸ŒëŸ¬ë¦¬  
   - **PEFT ë¼ì´ë¸ŒëŸ¬ë¦¬**: LoRA ë“± ë‹¤ì–‘í•œ ê¸°ë²•ì„ ê°„ë‹¨í•˜ê²Œ ì ìš©

### 6.2 ì£¼ì˜ì‚¬í•­

- **ë°ì´í„° í’ˆì§ˆ**: íŒŒì¸íŠœë‹ì— ì‚¬ìš©ë˜ëŠ” ë°ì´í„°ì˜ **ë„ë©”ì¸ ì í•©ì„±**ê³¼ **ë ˆì´ë¸” í’ˆì§ˆ**ì´ ë§¤ìš° ì¤‘ìš”  
- **ì˜¤ë²„í”¼íŒ… ë°©ì§€**: ë¬´ì‘ì • í•™ìŠµë¥ ì´ë‚˜ ì—í¬í¬ë¥¼ ë†’ì´ë©´, **ìƒì„± ëŠ¥ë ¥ì´ ë‹¨ìˆœí™”**ë˜ê±°ë‚˜ **ëª¨ë¸ í¸í–¥**ì´ ë°œìƒí•  ìˆ˜ ìˆìŒ  
- **ëª¨ë¸ í˜¸í™˜ì„±**: ì¼ë¶€ ëª¨ë¸ì€ ì•„í‚¤í…ì²˜ íŠ¹ì„±ìƒ íŒŒì¸íŠœë‹ì´ ì œí•œë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, **ê³µì‹ ë¬¸ì„œ**ë‚˜ **ì»¤ë®¤ë‹ˆí‹° ì •ë³´**ë¥¼ í™•ì¸

---

ì´ëŸ¬í•œ **LangChain**, **RAG**, **Vector DB**, **LLM Finetuning** ê¸°ë²•ë“¤ì„ ì¡°í•©í•˜ë©´,

1. **LLM í™œìš© íŒŒì´í”„ë¼ì¸**ì„ ì†ì‰½ê²Œ êµ¬ì„±í•  ìˆ˜ ìˆê³ ,  
2. **ì •í™•í•˜ê³  í’ë¶€í•œ ì§€ì‹**ì„ ê¸°ë°˜ìœ¼ë¡œ ëª¨ë¸ì˜ ì‘ë‹µì„ **ê°•í™”**í•˜ë©°,  
3. **íŠ¹ì • ë„ë©”ì¸**ì´ë‚˜ **ì—…ë¬´ í™˜ê²½**ì— ìµœì í™”ëœ **ì»¤ìŠ¤í…€ LLM**ì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì‹¤ë¬´ë‚˜ ì—°êµ¬ì—ì„œ LLMì„ í™œìš©í•  ë•Œ,  
- **LangChain**ìœ¼ë¡œ **ì›Œí¬í”Œë¡œìš°**ë¥¼ ì²´ê³„í™”í•˜ê³   
- **RAG**ë¥¼ í†µí•´ **ì‹¤ì‹œê°„ ì§€ì‹ ê²€ìƒ‰**ì„ ë”í•˜ë©°  
- **Vector DB**ë¡œ **ê²€ìƒ‰ ì„±ëŠ¥**ì„ ê·¹ëŒ€í™”í•˜ê³   
- **LLM Finetuning**ìœ¼ë¡œ **ë„ë©”ì¸ íŠ¹í™” ëª¨ë¸**ì„ ë§Œë“¤ë©´,  
ë”ìš± **íš¨ìœ¨ì **ì´ê³  **ê°•ë ¥**í•œ NLP ì†”ë£¨ì…˜ì„ êµ¬í˜„í•  ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤.

---

## ë…¸íŠ¸ë¶ ì‹œì—° ë‚´ìš©

ì´ ë…¸íŠ¸ë¶ì—ì„œëŠ” **ê°„ë‹¨í•œ ì˜ˆì‹œ ì½”ë“œ**ë¥¼ í†µí•´ ë¡œì»¬ í™˜ê²½ì—ì„œ ë‹¤ìŒì„ ì‚´í´ë´…ë‹ˆë‹¤:

1. **ë¡œì»¬ Llama ëª¨ë¸(ë˜ëŠ” ìœ ì‚¬ ëª¨ë¸) ë¡œë“œ ë° ê°„ë‹¨í•œ ì¶”ë¡ **  
2. **LangChain + Vector DBë¥¼ ì´ìš©í•œ RAG ì˜ˆì‹œ**  
3. (**ê°„ë‹¨ ë²„ì „**) **LLM íŒŒì¸íŠœë‹ ì˜ˆì‹œ**

pip install -r requirements.txt
  
í„°ë¯¸ë„ì—ì„œ ì‹¤í–‰í•˜ì—¬ í™˜ê²½ ì„¸íŒ…

ì˜ˆì‹œ: ë¡œì»¬ LLM ë¡œë“œ
ì•„ë˜ ì˜ˆì‹œëŠ” Hugging Face ëª¨ë¸ ì €ì¥ì†Œë¡œë¶€í„° Llama3.1-8bë¥¼ ë¡œì»¬ì— ë‹¤ìš´ë¡œë“œë°›ê³ ,
ê°„ë‹¨ ì¶”ë¡ ì„ ìˆ˜í–‰í•˜ëŠ” ì½”ë“œ ì˜ˆì‹œì…ë‹ˆë‹¤.

Apple ì‹¤ë¦¬ì½˜(M1, M4 ë“±)ì—ì„œëŠ” PyTorch `mps` ë””ë°”ì´ìŠ¤ê°€ ìë™ìœ¼ë¡œ ì¡íˆê±°ë‚˜ ìˆ˜ë™ ì„¤ì •ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.


---
í† í°(Token)ê³¼ í† í¬ë‚˜ì´ì €(Tokenizer)

ìì—°ì–´ ì²˜ë¦¬(NLP)ì—ì„œ **í† í°í™”(Tokenization)**ëŠ” í…ìŠ¤íŠ¸ë¥¼ íŠ¹ì • ë‹¨ìœ„ë¡œ ë¶„í• í•˜ëŠ” ë§¤ìš° ì¤‘ìš”í•œ ì „ì²˜ë¦¬ ê³¼ì •ì…ë‹ˆë‹¤. ê·¸ ê³¼ì •ì—ì„œ í™œìš©ë˜ëŠ” **í† í¬ë‚˜ì´ì €(Tokenizer)**ì— ëŒ€í•´ ìì„¸íˆ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤.

1. í† í°(Token)ì´ë€?

**í† í°(Token)**ì€ í…ìŠ¤íŠ¸(ë¬¸ì¥, ë¬¸ë‹¨ ë“±)ë¥¼ ì‘ì€ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë‚˜ëˆˆ ê²°ê³¼ë¬¼ì…ë‹ˆë‹¤. ì–´ë–¤ ê¸°ì¤€ìœ¼ë¡œ ë‚˜ëˆ„ëŠëƒì— ë”°ë¼ ë‹¤ì–‘í•œ í† í° ë‹¨ìœ„ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

1.1 ë‹¨ì–´ ë‹¨ìœ„
	â€¢	ê³µë°±(whitespace)ì´ë‚˜ êµ¬ë‘ì (punctuation)ì„ ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¦¬í•˜ëŠ” ê°„ë‹¨í•œ ë°©ì‹
	â€¢	ì˜ˆ) â€œë‚˜ëŠ” í•™êµì— ê°„ë‹¤.â€ â†’ ["ë‚˜ëŠ”", "í•™êµì—", "ê°„ë‹¤."]

1.2 í˜•íƒœì†Œ ë‹¨ìœ„
	â€¢	í•œêµ­ì–´ì—ì„œ ìì£¼ ì‚¬ìš©í•˜ëŠ” ë°©ì‹
	â€¢	ì˜ˆ) â€œë‚˜ëŠ” í•™êµì— ê°„ë‹¤.â€ â†’ ["ë‚˜", "ëŠ”", "í•™êµ", "ì—", "ê°€", "á„‚ë‹¤", "."]

1.3 ì„œë¸Œì›Œë“œ(Subword) ë‹¨ìœ„
	â€¢	BPE, WordPiece, SentencePiece ë“±ì˜ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•˜ì—¬ ë‹¨ì–´ë³´ë‹¤ ì‘ì€ ë‹¨ìœ„ë¡œ ë‚˜ëˆ”
	â€¢	ì˜ˆ) â€œunhappyâ€ â†’ ["un", "happy"]
	â€¢	ì˜ˆ) â€œunbelievableâ€ â†’ ["un", "believable"]

2. í† í¬ë‚˜ì´ì €(Tokenizer)ë€?

**í† í¬ë‚˜ì´ì €(Tokenizer)**ëŠ” í…ìŠ¤íŠ¸ë¥¼ íŠ¹ì • ê·œì¹™ í˜¹ì€ ì•Œê³ ë¦¬ì¦˜ì„ ì‚¬ìš©í•´ í† í°ìœ¼ë¡œ ë¶„í• í•˜ëŠ” ë„êµ¬(ë˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬)ì…ë‹ˆë‹¤.

2.1 ê·œì¹™ ê¸°ë°˜ í† í¬ë‚˜ì´ì €
	â€¢	ê³µë°±, êµ¬ë‘ì , íŠ¹ì • íŒ¨í„´(ì •ê·œ í‘œí˜„ì‹) ë“±ì„ ê¸°ì¤€ìœ¼ë¡œ ë¯¸ë¦¬ ì •í•´ì§„ ê·œì¹™ì— ë”°ë¼ ë¶„ë¦¬
	â€¢	ì˜ˆ) split(), ì •ê·œì‹(Regex)ì„ í†µí•œ ë‹¨ìˆœ ë¶„í• 

2.2 í•™ìŠµ ê¸°ë°˜ í† í¬ë‚˜ì´ì €
	â€¢	í›ˆë ¨ ë°ì´í„°(ì½”í¼ìŠ¤)ì— ë§ì¶° í† í° ì‚¬ì „ì„ ìë™ ìƒì„±í•˜ê³ , í•´ë‹¹ ê·œì¹™ì„ í•™ìŠµ
	â€¢	BPE(Byte-Pair Encoding), WordPiece, SentencePiece ë“±ì´ ëŒ€í‘œì 
	â€¢	ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(ì˜ˆ: BERT, GPT ë“±)ì—ì„œë„ ì‚¬ìš©

3. í† í°í™” ê³¼ì •ì´ ì¤‘ìš”í•œ ì´ìœ 
	1.	ì •í™•ë„ í–¥ìƒ
	â€¢	ì˜¬ë°”ë¥¸ ë‹¨ìœ„ë¡œ í…ìŠ¤íŠ¸ë¥¼ ë‚˜ëˆ„ì–´ì•¼ í˜•íƒœì†Œ ë¶„ì„, í’ˆì‚¬ íƒœê¹… ë“± í›„ì† ì‘ì—…ì˜ ì •í™•ë„ê°€ ë†’ì•„ì§‘ë‹ˆë‹¤.
	2.	ì–´íœ˜ ì‚¬ì „ ê´€ë¦¬
	â€¢	ë‹¨ì–´ ë‹¨ìœ„ë¡œë§Œ ë‚˜ëˆ„ë©´ ì–´íœ˜ ì‚¬ì „ì´ ë„ˆë¬´ ì»¤ì§ˆ ìˆ˜ ìˆìœ¼ë‚˜, ì„œë¸Œì›Œë“œ ë°©ì‹ì„ ì‚¬ìš©í•˜ë©´ í¬ê·€ ë‹¨ì–´ë‚˜ ì‹ ì¡°ì–´ë„ íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
	3.	ëª¨ë¸ ì„±ëŠ¥ ê·¹ëŒ€í™”
	â€¢	í† í°í™”ê°€ ì˜ëª»ë˜ë©´ ëª¨ë¸ì´ í•™ìŠµì— ì–´ë ¤ì›€ì„ ê²ªê±°ë‚˜, ì¶”ë¡  ì‹œ ì„±ëŠ¥ì´ ë–¨ì–´ì§‘ë‹ˆë‹¤.
	â€¢	BERTë‚˜ GPT ê°™ì€ ëª¨ë¸ë“¤ë„ ì¼ê´€ëœ í† í¬ë‚˜ì´ì§• ê·œì¹™ì„ ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµë©ë‹ˆë‹¤.

4. í•œêµ­ì–´ í† í¬ë‚˜ì´ì§• ì‹œ ê³ ë ¤ì‚¬í•­
	1.	í˜•íƒœì†Œ ë¶„ì„ì˜ í•„ìš”ì„±
	â€¢	í•œêµ­ì–´ëŠ” ì¡°ì‚¬, ì–´ë¯¸ ë³€í™” ë“±ì´ ë‹¤ì–‘í•´ ë‹¨ìˆœ ê³µë°±ìœ¼ë¡œë§Œ í† í°í™”í•˜ê¸° ì–´ë µìŠµë‹ˆë‹¤.
	2.	ì–´ì ˆ ë‚´ë¶€ì˜ ë‹¤ì–‘í•œ ìš”ì†Œ
	â€¢	â€œí•™êµì—â€ â†’ ["í•™êµ", "ì—"]
	â€¢	â€œí•™êµì—ì„œâ€ â†’ ["í•™êµ", "ì—ì„œ"]
	â€¢	ì¡°ì‚¬ì™€ ì–´ê°„ ë“±ì„ ë¶„ë¦¬í•´ì¤˜ì•¼ ì›í•˜ëŠ” ì •í™•ë„ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
	3.	ë¶ˆê·œì¹™í•œ ë„ì–´ì“°ê¸°
	â€¢	ë„ì–´ì“°ê¸°ë¥¼ ì •í™•íˆ ì§€í‚¤ì§€ ì•ŠëŠ” ê²½ìš°ê°€ ë§ìœ¼ë¯€ë¡œ ê·œì¹™ ê¸°ë°˜ë§Œìœ¼ë¡œëŠ” ì²˜ë¦¬ì— í•œê³„ê°€ ìˆìŠµë‹ˆë‹¤.

5. ëŒ€í‘œì ì¸ í† í¬ë‚˜ì´ì € ì˜ˆì‹œ
	â€¢	NLTK (ì˜ì–´)
	â€¢	íŒŒì´ì¬ì—ì„œ ê°€ì¥ ë§ì´ ì‚¬ìš©ë˜ëŠ” NLP ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¤‘ í•˜ë‚˜
	â€¢	ë‹¨ì–´ í† í°í™”, ë¬¸ì¥ í† í°í™”, ìŠ¤í†±ì›Œë“œ ì œê±° ë“± ë‹¤ì–‘í•œ ê¸°ëŠ¥ ì œê³µ
	â€¢	KoNLPy (í•œêµ­ì–´)
	â€¢	íŠ¸ìœ„í„°(Twitter), ê¼¬ê¼¬ë§ˆ(Kkma), í•œë‚˜ëˆ”(Hannanum) ë“± ì—¬ëŸ¬ í˜•íƒœì†Œ ë¶„ì„ê¸° ì—°ë™ ê°€ëŠ¥
	â€¢	í˜•íƒœì†Œ ë¶„ì„, í’ˆì‚¬ íƒœê¹… ë“± í•œêµ­ì–´ ì „ìš© ì²˜ë¦¬ ê¸°ëŠ¥
	â€¢	BPE, SentencePiece, WordPiece
	â€¢	ì„œë¸Œì›Œë“œ(subword) ê¸°ë°˜ í† í¬ë‚˜ì´ì €
	â€¢	ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(ì˜ˆ: BERT, GPT, RoBERTa ë“±)ì—ì„œ ì‚¬ìš©

6. ì •ë¦¬
	â€¢	í† í°(Token)
	â€¢	í…ìŠ¤íŠ¸ë¥¼ ì‘ì€ ë‹¨ìœ„(ë‹¨ì–´, í˜•íƒœì†Œ, ì„œë¸Œì›Œë“œ ë“±)ë¡œ ë¶„í• í•œ ê²°ê³¼ë¬¼
	â€¢	í† í¬ë‚˜ì´ì €(Tokenizer)
	â€¢	í…ìŠ¤íŠ¸ë¥¼ ì›í•˜ëŠ” ê¸°ì¤€ìœ¼ë¡œ ë¶„í• í•˜ê¸° ìœ„í•œ ë„êµ¬ ë˜ëŠ” ì•Œê³ ë¦¬ì¦˜
	â€¢	í† í°í™”(Tokenization)
	â€¢	ìì—°ì–´ ì²˜ë¦¬ì˜ í•µì‹¬ ì „ì²˜ë¦¬ ë‹¨ê³„ì´ë©°, ì˜¬ë°”ë¥¸ í† í°í™”ë¥¼ í†µí•´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë†’ì´ê³  ì–´íœ˜ ì‚¬ì „ì„ íš¨ìœ¨ì ìœ¼ë¡œ ê´€ë¦¬í•  ìˆ˜ ìˆìŒ
	â€¢	í•œêµ­ì–´ëŠ” ì¡°ì‚¬, ì–´ë¯¸ ë³€í™” ë“±ì´ ë³µì¡í•˜ë¯€ë¡œ í˜•íƒœì†Œ ë¶„ì„ ë˜ëŠ” ì„œë¸Œì›Œë“œ ë°©ì‹ì„ ê³ ë ¤í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì 

ê²°ë¡ ì ìœ¼ë¡œ, ì ì ˆí•œ í† í¬ë‚˜ì´ì§•ì„ í†µí•´ ë°ì´í„° í’ˆì§ˆê³¼ ëª¨ë¸ ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. NLP í”„ë¡œì íŠ¸ì— ë§ëŠ” í† í¬ë‚˜ì´ì €ë¥¼ ì„ íƒí•˜ê³ , ì–¸ì–´ë³„ íŠ¹ì„±ì„ ë°˜ì˜í•œ í† í°í™” ì „ëµì„ ìˆ˜ë¦½í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.


```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# Llama ëª¨ë¸ ì˜ˆì‹œ (ì‹¤ì œ ì‚¬ìš© ì‹œ, Hugging Faceì—ì„œ ì ‘ê·¼ ê¶Œí•œì´ í•„ìš”í•˜ê±°ë‚˜ ëª¨ë¸ ì´ë¦„ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.)
#model_name = "meta-llama/Llama-3.1-8B"
model_name="../models/Llama-3.1-8b"  
# mps ë””ë°”ì´ìŠ¤ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
device = torch.device("mps") if torch.backends.mps.is_available() else torch.device("cpu")
print("Using device:", device)

# í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map={"": device})

# ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ìƒì„± ì˜ˆì‹œ
prompt = "Hello, how are you?"
inputs = tokenizer(prompt, return_tensors="pt").to(device)
with torch.no_grad():
    outputs = model.generate(**inputs, max_new_tokens=50)
generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
print("Generated text:")
print(generated_text)

```

    Using device: mps



    Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]


    Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.


    Generated text:
    Hello, how are you? Today we are going to talk about the new generation of smart watches, which are very popular in the world of technology and are very useful for our daily life.
    The smart watch is a device that is in charge of tracking the physical activity of its user


ì„¤ëª…:

- torch_dtype=torch.float16: Apple MPSì—ì„œëŠ” float16ì´ ìµœì ì…ë‹ˆë‹¤.
- device_map={"": device}: ëª¨ë¸ì„ ìë™ìœ¼ë¡œ MPS(GPU) ë˜ëŠ” CPUì— ë°°ì¹˜í•©ë‹ˆë‹¤.
- max_new_tokens=50: ì‘ë‹µ ê¸¸ì´ë¥¼ 50ê°œ í† í°ìœ¼ë¡œ ì œí•œí•©ë‹ˆë‹¤.

Llama 3.1 8B ëª¨ë¸ + LangChain + Vector DB(RAG) ì ìš©
Llama 3.1 8B ëª¨ë¸ì„ LangChain ë° ChromaDB(Vector DB)ì™€ í•¨ê»˜ ì‚¬ìš©í•˜ì—¬ Retrieval-Augmented Generation(RAG) ì„ ìˆ˜í–‰í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.

2.1 LangChain & ChromaDB ì„¤ì¹˜

```bash
pip install langchain chromadb faiss-cpu sentence-transformers
```

2.2 RAG ì ìš© ì½”ë“œ


```python
# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
from transformers import pipeline  # <-- ì—¬ê¸°ì— ì¶”ê°€
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.docstore.document import Document
from langchain.chains import RetrievalQA
from langchain.llms import HuggingFacePipeline

# 1) ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
embedding_model = "sentence-transformers/all-MiniLM-L6-v2"
embeddings = HuggingFaceEmbeddings(model_name=embedding_model)

# ì˜ˆì œ ë¬¸ì„œ
texts = [
    "LangChainì€ LLMì„ ì²´ì¸ìœ¼ë¡œ ì—°ê²°í•˜ëŠ” í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.",
    "RAGëŠ” Retrieval-Augmented Generationì˜ ì•½ì–´ì…ë‹ˆë‹¤.",
    "Vector DBëŠ” ë¬¸ì„œ ì„ë² ë”© ë²¡í„°ë¥¼ ê²€ìƒ‰í•˜ê¸° ìœ„í•œ ë°ì´í„°ë² ì´ìŠ¤ì…ë‹ˆë‹¤."
]
documents = [Document(page_content=t) for t in texts]

# 2) Chroma VectorStore ì´ˆê¸°í™”
vectorstore = Chroma.from_documents(documents, embedding=embeddings, collection_name="example_collection")

# 3) Llama 3.1 8B ëª¨ë¸ì„ LangChainì— ì—°ê²°
generator_pipeline = pipeline(
    "text-generation",
    model=model,  # Llama 3.1 8B ëª¨ë¸
    tokenizer=tokenizer
)
llm = HuggingFacePipeline(pipeline=generator_pipeline)

# 4) RetrievalQA ì²´ì¸ ìƒì„±
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=vectorstore.as_retriever()
)

# 5) ì§ˆë¬¸ ì…ë ¥ ë° RAG ê¸°ë°˜ ë‹µë³€ ìƒì„±
query = "RAGëŠ” ë¬´ì—‡ì¸ê°€ìš”?"
answer = qa_chain.run(query)
print(f"Q: {query}\nA: {answer}")

```

    Device set to use mps
    /var/folders/z8/94fh0xbx5cv85y4f8dm4_nch0000gn/T/ipykernel_1491/842959494.py:30: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.
      llm = HuggingFacePipeline(pipeline=generator_pipeline)
    /var/folders/z8/94fh0xbx5cv85y4f8dm4_nch0000gn/T/ipykernel_1491/842959494.py:41: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.
      answer = qa_chain.run(query)
    Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.


    Q: RAGëŠ” ë¬´ì—‡ì¸ê°€ìš”?
    A: Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.
    
    RAGëŠ” Retrieval-Augmented Generationì˜ ì•½ì–´ì…ë‹ˆë‹¤.
    
    RAGëŠ” Retrieval-Augmented Generationì˜ ì•½ì–´ì…ë‹ˆë‹¤.
    
    RAGëŠ” Retrieval-Augmented Generationì˜ ì•½ì–´ì…ë‹ˆë‹¤.
    
    LangChainì€ LLMì„ ì²´ì¸ìœ¼ë¡œ ì—°ê²°í•˜ëŠ” í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.
    
    Question: RAGëŠ” ë¬´ì—‡ì¸ê°€ìš”?
    Helpful Answer: Retrieval-Augmented Generationì˜ ì•½ì–´ì…ë‹ˆë‹¤.


## **ì„ë² ë”© ëª¨ë¸(Embedding Model)ì´ë€?**

### 1ï¸âƒ£ **ê°œë…**
ì„ë² ë”© ëª¨ë¸(Embedding Model)ì´ë€, ë°ì´í„°ë¥¼ **ê³ ì •ëœ í¬ê¸°ì˜ ë²¡í„°(vector) í˜•íƒœë¡œ ë³€í™˜í•˜ëŠ” ëª¨ë¸**ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.  
í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, ì˜¤ë””ì˜¤ ë“± ë‹¤ì–‘í•œ ë°ì´í„° ìœ í˜•ì„ **ìˆ˜ì¹˜ ë²¡í„°ë¡œ ë³€í™˜**í•˜ì—¬ ì»´í“¨í„°ê°€ ì´í•´í•˜ê³  ì—°ì‚°í•  ìˆ˜ ìˆë„ë¡ ë§Œë“¤ì–´ ì¤ë‹ˆë‹¤.

ì¦‰, **ì„ë² ë”©(Embedding)**ì€ ê³ ì°¨ì› ë°ì´í„°(ì˜ˆ: ë‹¨ì–´, ë¬¸ì¥, ë¬¸ì„œ, ì´ë¯¸ì§€ ë“±)ë¥¼ **ë°€ì§‘ ë²¡í„°(Dense Vector)**ë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •ì´ë©°,  
ì´ ë²¡í„°ë¥¼ ìƒì„±í•˜ëŠ” ëª¨ë¸ì„ **ì„ë² ë”© ëª¨ë¸(Embedding Model)**ì´ë¼ê³  í•©ë‹ˆë‹¤.

---

### 2ï¸âƒ£ **ì™œ ì„ë² ë”©ì´ í•„ìš”í•œê°€?**
ì»´í“¨í„°ëŠ” ìˆ«ìë§Œ ì´í•´í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ìì—°ì–´(NLP)ë‚˜ ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ ì§ì ‘ ì²˜ë¦¬í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.  
ë”°ë¼ì„œ, ë‹¤ìŒê³¼ ê°™ì€ ì´ìœ ë¡œ **ì„ë² ë”© ëª¨ë¸**ì´ í•„ìš”í•©ë‹ˆë‹¤.

âœ… **ë¬¸ìì—´ì„ ìˆ«ìë¡œ ë³€í™˜**: í…ìŠ¤íŠ¸ë¥¼ ìˆ«ì ë²¡í„°ë¡œ ë³€í™˜í•´ì•¼ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆìŒ  
âœ… **ìœ ì‚¬í•œ ë°ì´í„°ë¼ë¦¬ ê°€ê¹Œìš´ ë²¡í„° ê°’ì„ ê°€ì§**: ì˜ë¯¸ì ìœ¼ë¡œ ë¹„ìŠ·í•œ ë‹¨ì–´ëŠ” ìœ ì‚¬í•œ ë²¡í„°ë¡œ ë³€í™˜ë¨  
âœ… **ê³ ì°¨ì› ë°ì´í„°ë¥¼ ì €ì°¨ì›ìœ¼ë¡œ ì••ì¶•**: ì°¨ì›ì´ ë†’ì€ ë°ì´í„°ë¥¼ ë‚®ì€ ì°¨ì›ì˜ ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ ì—°ì‚°ì„ ìµœì í™”  

> ì˜ˆë¥¼ ë“¤ì–´, "ê³ ì–‘ì´(cat)"ì™€ "ê°•ì•„ì§€(dog)"ëŠ” **ë¹„ìŠ·í•œ ì˜ë¯¸**ë¥¼ ê°€ì§€ë¯€ë¡œ, ì„ë² ë”© ë²¡í„° ê³µê°„ì—ì„œ ì„œë¡œ ê°€ê¹Œìš´ ìœ„ì¹˜ì— ìˆìŒ.  
> ë°˜ë©´, "ê³ ì–‘ì´(cat)"ì™€ "ìë™ì°¨(car)"ëŠ” ê´€ë ¨ì„±ì´ ì ìœ¼ë¯€ë¡œ ë²¡í„° ê³µê°„ì—ì„œ ë©€ë¦¬ ë–¨ì–´ì ¸ ìˆìŒ.

---

### 3ï¸âƒ£ **ì„ë² ë”© ëª¨ë¸ì˜ ì¢…ë¥˜**
ì„ë² ë”© ëª¨ë¸ì€ **í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, ì˜¤ë””ì˜¤** ë“± ë‹¤ì–‘í•œ ë°ì´í„°ë¥¼ ë²¡í„°í™”í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.  
ëŒ€í‘œì ì¸ ì„ë² ë”© ëª¨ë¸ ì¢…ë¥˜ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

#### ğŸ”¹ **(1) í…ìŠ¤íŠ¸ ì„ë² ë”© (Word/Sentence Embedding)**
- ìì—°ì–´ ì²˜ë¦¬(NLP)ì—ì„œ ë‹¨ì–´, ë¬¸ì¥, ë¬¸ì„œë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ëª¨ë¸
- **ì‚¬ìš©ì²˜**: ì±—ë´‡, ê²€ìƒ‰, ì¶”ì²œ ì‹œìŠ¤í…œ, ë¬¸ì„œ ë¶„ë¥˜, RAG(Retrieval-Augmented Generation)

ğŸ“Œ **ëŒ€í‘œì ì¸ ëª¨ë¸**
- `Word2Vec`: ë‹¨ì–´ë¥¼ ë²¡í„°í™”í•˜ëŠ” ëŒ€í‘œì ì¸ ëª¨ë¸
- `GloVe(Global Vectors for Word Representation)`: ë‹¨ì–´ ê°„ì˜ í†µê³„ì  ì—°ê´€ì„±ì„ ë°˜ì˜í•œ ë²¡í„°
- `FastText`: Word2Vec ê°œì„  ë²„ì „ (ë¶€ë¶„ ë‹¨ì–´ê¹Œì§€ ì„ë² ë”© ê°€ëŠ¥)
- `BERT Embedding`: ë¬¸ë§¥ì„ ê³ ë ¤í•˜ì—¬ ë‹¨ì–´/ë¬¸ì¥ì„ ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” ê°•ë ¥í•œ ëª¨ë¸
- `Sentence-BERT (SBERT)`: ë¬¸ì¥ ë‹¨ìœ„ ì„ë² ë”©ì„ ìƒì„±í•˜ëŠ” ëª¨ë¸
- `sentence-transformers/all-MiniLM-L6-v2`: ê°€ë³ê³  ë¹ ë¥¸ ë¬¸ì¥ ì„ë² ë”© ëª¨ë¸ (LangChain, RAGì—ì„œ ë§ì´ ì‚¬ìš©ë¨)

âœ… **ì˜ˆì œ**: `sentence-transformers`ë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ì„ë² ë”© ë²¡í„° ìƒì„±  
```python
from sentence_transformers import SentenceTransformer

# SBERT ê¸°ë°˜ ë¬¸ì¥ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
model = SentenceTransformer("all-MiniLM-L6-v2")

# ì˜ˆì œ ë¬¸ì¥
sentences = ["ê³ ì–‘ì´ëŠ” ê·€ì—½ë‹¤.", "ê°•ì•„ì§€ëŠ” ì¶©ì„±ìŠ¤ëŸ½ë‹¤.", "ìë™ì°¨ëŠ” ë¹ ë¥´ë‹¤."]

# ì„ë² ë”© ë³€í™˜ (ë²¡í„° ìƒì„±)
embeddings = model.encode(sentences)

# ì¶œë ¥ (ì²« ë²ˆì§¸ ë¬¸ì¥ì˜ ë²¡í„° ê°’)
print(embeddings[0])
```

---

#### ğŸ”¹ **(2) ì´ë¯¸ì§€ ì„ë² ë”© (Image Embedding)**
- ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ ë¹„ìŠ·í•œ ì´ë¯¸ì§€ ê²€ìƒ‰, ê°ì²´ ì¸ì‹ ë“±ì— í™œìš©
- **ì‚¬ìš©ì²˜**: ì´ë¯¸ì§€ ê²€ìƒ‰ ì‹œìŠ¤í…œ, ìŠ¤íƒ€ì¼ ì¶”ì²œ, ì»´í“¨í„° ë¹„ì „

ğŸ“Œ **ëŒ€í‘œì ì¸ ëª¨ë¸**
- `ResNet-50`, `EfficientNet`, `CLIP`: ì´ë¯¸ì§€ ë¶„ë¥˜ ë° íŠ¹ì§• ë²¡í„° ì¶”ì¶œ
- `DINOv2`: ìµœê·¼ Metaì—ì„œ ë°œí‘œí•œ ì´ë¯¸ì§€ ì„ë² ë”© ëª¨ë¸
- `OpenAI CLIP`: í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ë¥¼ ë™ì¼í•œ ë²¡í„° ê³µê°„ì— ë§¤í•‘í•˜ëŠ” ëª¨ë¸ (ì˜ˆ: "ê°•ì•„ì§€ ì‚¬ì§„" â†’ ê°•ì•„ì§€ ì´ë¯¸ì§€ ë²¡í„°ì™€ ìœ ì‚¬í•œ ìœ„ì¹˜)

âœ… **ì˜ˆì œ**: `CLIP`ì„ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ ì„ë² ë”© ë²¡í„° ìƒì„±
```python
import torch
import clip
from PIL import Image

# CLIP ëª¨ë¸ ë¡œë“œ
device = "cuda" if torch.cuda.is_available() else "cpu"
model, preprocess = clip.load("ViT-B/32", device=device)

# ì´ë¯¸ì§€ ë¡œë“œ ë° ë³€í™˜
image = preprocess(Image.open("dog.jpg")).unsqueeze(0).to(device)

# ì„ë² ë”© ë²¡í„° ì¶”ì¶œ
with torch.no_grad():
    image_features = model.encode_image(image)

print(image_features.shape)  # [1, 512] í¬ê¸°ì˜ ë²¡í„° ì¶œë ¥
```

---

#### ğŸ”¹ **(3) ì˜¤ë””ì˜¤ ì„ë² ë”© (Audio Embedding)**
- ì˜¤ë””ì˜¤ ë°ì´í„°ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ ìŒì„± ì¸ì‹, ê°ì • ë¶„ì„ ë“±ì— í™œìš©
- **ì‚¬ìš©ì²˜**: ìŒì„± ì¸ì‹(Speech-to-Text), ê°ì • ë¶„ì„, ë…¸ì´ì¦ˆ í•„í„°ë§, ìŒì•… ì¶”ì²œ ì‹œìŠ¤í…œ

ğŸ“Œ **ëŒ€í‘œì ì¸ ëª¨ë¸**
- `MFCC (Mel-Frequency Cepstral Coefficients)`: ìŒì„± ë°ì´í„°ë¥¼ íŠ¹ì§• ë²¡í„°ë¡œ ë³€í™˜
- `wav2vec 2.0 (by Facebook)`: ìŒì„±ì„ ë²¡í„°í™”í•˜ê³  í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ëŠ” ëª¨ë¸
- `Whisper (by OpenAI)`: ë‹¤êµ­ì–´ ìŒì„± ì¸ì‹ ë° ì„ë² ë”© ëª¨ë¸

âœ… **ì˜ˆì œ**: `librosa`ë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜¤ë””ì˜¤ ì„ë² ë”© ë²¡í„° ìƒì„±
```python
import librosa

# ì˜¤ë””ì˜¤ íŒŒì¼ ë¡œë“œ
y, sr = librosa.load("speech.wav")

# MFCC íŠ¹ì§• ë²¡í„° ì¶”ì¶œ
mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)

# ì¶œë ¥ (ì²« ë²ˆì§¸ í”„ë ˆì„ì˜ MFCC ë²¡í„°)
print(mfccs[:, 0])
```

---

### 4ï¸âƒ£ **ì„ë² ë”© ë²¡í„°ì˜ í™œìš©**
ì„ë² ë”© ëª¨ë¸ì„ í™œìš©í•˜ë©´ **ë¬¸ì„œ ê²€ìƒ‰, ì±—ë´‡, ì¶”ì²œ ì‹œìŠ¤í…œ, RAG ëª¨ë¸** ë“±ì—ì„œ ê°•ë ¥í•œ ì„±ëŠ¥ì„ ë°œíœ˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

âœ… **ë¬¸ì„œ ê²€ìƒ‰ ì‹œìŠ¤í…œ**  
- ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì§ˆë¬¸ì„ ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ **ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œ**ë¥¼ ê²€ìƒ‰  
- ì˜ˆ: **LangChain + ChromaDB** ë¥¼ í™œìš©í•œ **RAG (Retrieval-Augmented Generation)**  

âœ… **ì±—ë´‡ ë° LLM ì‘ìš©**  
- ì±—ë´‡ì´ ë‹µë³€ì„ ìƒì„±í•  ë•Œ **ì„ë² ë”© ë²¡í„° ê¸°ë°˜ ê²€ìƒ‰**ì„ ìˆ˜í–‰  
- ì˜ˆ: ì‚¬ìš©ìì˜ ì…ë ¥ì„ ì„ë² ë”© í›„, **ê°€ì¥ ê´€ë ¨ì„±ì´ ë†’ì€ ë‹µë³€ì„ DBì—ì„œ ê²€ìƒ‰í•˜ì—¬ ì‘ë‹µ**  

âœ… **ì¶”ì²œ ì‹œìŠ¤í…œ**  
- Netflix, Spotify ê°™ì€ ì¶”ì²œ ì‹œìŠ¤í…œì—ì„œ **ì‚¬ìš©ìì˜ ì·¨í–¥ì„ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜**  
- ì˜ˆ: ì‚¬ìš©ìê°€ ì‹œì²­í•œ ì˜í™”/ìŒì•…ì˜ ì„ë² ë”© ë²¡í„°ë¥¼ í™œìš©í•˜ì—¬ **ë¹„ìŠ·í•œ ì½˜í…ì¸  ì¶”ì²œ**  

âœ… **ì˜ë£Œ ë° ìƒë¬¼ì •ë³´í•™**  
- ìœ ì „ì ë°ì´í„°, ì˜í•™ ë…¼ë¬¸, ë‹¨ë°±ì§ˆ êµ¬ì¡° ë°ì´í„°ë¥¼ ë²¡í„°í™”í•˜ì—¬ ë¶„ì„  
- ì˜ˆ: ì‹ ì•½ ê°œë°œ, ìœ ì „ì²´ ë¶„ì„  

---

### 5ï¸âƒ£ **ê²°ë¡ **
ì„ë² ë”© ëª¨ë¸ì€ ë°ì´í„°(í…ìŠ¤íŠ¸, ì´ë¯¸ì§€, ì˜¤ë””ì˜¤)ë¥¼ **ë²¡í„° í‘œí˜„ìœ¼ë¡œ ë³€í™˜í•˜ì—¬** ìœ ì‚¬ë„ ê²€ìƒ‰, ì±—ë´‡, ì¶”ì²œ ì‹œìŠ¤í…œ ë“± ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ í™œìš©ë©ë‹ˆë‹¤.  
ìµœê·¼ì—ëŠ” **Transformer ê¸°ë°˜ì˜ ê°•ë ¥í•œ ì„ë² ë”© ëª¨ë¸(BERT, CLIP, wav2vec 2.0 ë“±)ì´ ë“±ì¥**í•˜ë©´ì„œ, **ë” ì •êµí•œ ì˜ë¯¸ í‘œí˜„ì´ ê°€ëŠ¥**í•´ì¡ŒìŠµë‹ˆë‹¤.

> ğŸš€ **LangChain + Vector DB (Chroma, FAISS) + LLM(RAG)** ì„ í™œìš©í•˜ë©´ ë”ìš± ê°•ë ¥í•œ AI ì‘ìš© ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!

---

### 3)Llama 3.1 8B ëª¨ë¸ Finetuning (LoRA ì ìš©)
Llama 3.1 8B ëª¨ë¸ì„ LoRA (Low-Rank Adaptation) ë°©ì‹ìœ¼ë¡œ íŒŒì¸íŠœë‹í•˜ë ¤ë©´ PEFT (Parameter-Efficient Fine-Tuning) ë¥¼ ì‚¬ìš©í•˜ë©´ ë©ë‹ˆë‹¤.

3.1 LoRA ì„¤ì¹˜

```bash
pip install peft bitsandbytes datasets
```

3.2 LoRA ì ìš© ì½”ë“œ


```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer
from peft import LoraConfig, get_peft_model
from datasets import Dataset

# ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
model_name = "../models/Llama-3.1-8b"
tokenizer = AutoTokenizer.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
print(f"Using device: {device}")

# ëª¨ë¸ ë¡œë“œ
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,  # âœ… float16 ëŒ€ì‹  bfloat16 ì‚¬ìš©
    device_map="auto"
).to(device)  # âœ… ëª…ì‹œì ìœ¼ë¡œ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™

# LoRA ì„¤ì •
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.1,
    bias="none",
    target_modules=["q_proj", "v_proj"]  # Llama ëª¨ë¸ì˜ í•µì‹¬ ê°€ì¤‘ì¹˜ ì¡°ì •
)

# LoRA ëª¨ë¸ ì ìš©
lora_model = get_peft_model(model, lora_config)

# âœ… ì…ë ¥ ë°ì´í„° ì¤€ë¹„
train_texts = [
    "Question: Llama ëª¨ë¸ì€?\nAnswer: Metaì˜ ëŒ€í˜• ì–¸ì–´ ëª¨ë¸ì…ë‹ˆë‹¤.",
    "Question: RAGë€?\nAnswer: ê²€ìƒ‰ ê¸°ë°˜ ìƒì„± ëª¨ë¸ì…ë‹ˆë‹¤."
]

# âœ… í† í°í™” í•¨ìˆ˜ ìˆ˜ì •
def tokenize_fn(text):
    tokenized = tokenizer(text, truncation=True, padding="max_length", max_length=64)
    return {
        "input_ids": tokenized["input_ids"],
        "attention_mask": tokenized["attention_mask"],
        "labels": tokenized["input_ids"]  # GPT ëª¨ë¸ì€ labels=input_ids ì‚¬ìš©
    }

# âœ… ë°ì´í„°ì…‹ ë³€í™˜ (Hugging Face Dataset)
train_dataset = Dataset.from_dict({"text": train_texts})
train_dataset = train_dataset.map(lambda x: tokenize_fn(x["text"]), batched=True, remove_columns=["text"])

# âœ… TrainingArguments ì„¤ì • (disable removal of unused columns)
training_args = TrainingArguments(
    output_dir="finetuned-llama3",
    per_device_train_batch_size=1,
    num_train_epochs=1,
    save_steps=10,
    logging_steps=5,
    remove_unused_columns=False,  # <--- Added line to fix the error
)

# âœ… Trainer ì„¤ì •
trainer = Trainer(
    model=lora_model,
    args=training_args,
    train_dataset=train_dataset,
)

# âœ… í•™ìŠµ ì‹œì‘
trainer.train()

# âœ… í•™ìŠµ ì™„ë£Œ í›„ ëª¨ë¸ ì €ì¥
trainer.save_model("finetuned-llama3")

# Save the adapter checkpoint (this will save adapter_config.json, adapter weights, etc.)
lora_model.save_pretrained("finetuned-llama3")
# Optionally, save the tokenizer as well.
tokenizer.save_pretrained("finetuned-llama3")
print("Adapter and tokenizer saved successfully!")


print("Finetuning Complete!")

```

    Using device: mps



    Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]


    'NoneType' object has no attribute 'cadam32bit_grad_fp32'


    /opt/anaconda3/envs/guide/lib/python3.9/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.
      warn("The installed version of bitsandbytes was compiled without GPU support. "



    Map:   0%|          | 0/2 [00:00<?, ? examples/s]




    <div>

      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>
      [2/2 00:00, Epoch 1/1]
    </div>
    <table border="1" class="dataframe">
  <thead>
 <tr style="text-align: left;">
      <th>Step</th>
      <th>Training Loss</th>
    </tr>
  </thead>
  <tbody>
  </tbody>
</table><p>


    Adapter and tokenizer saved successfully!
    Finetuning Complete!


LoRAë¥¼ í™œìš©í•˜ëŠ” ì´ìœ :

- ì¼ë°˜ì ì¸ Llama 3.1 8B ëª¨ë¸ í•™ìŠµì€ RAM 100GB ì´ìƒ í•„ìš”
- LoRAë¥¼ ì‚¬ìš©í•˜ë©´ íŠ¹ì • ê°€ì¤‘ì¹˜(Q, V í”„ë¡œì ì…˜)ë§Œ ì¡°ì •í•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì¤„ì„
- M4 Pro(64GB RAM) í™˜ê²½ì—ì„œë„ ì‹¤í–‰ ê°€ëŠ¥

### 4) ìš”ì•½
- Llama 3.1 8B ëª¨ë¸ ì‹¤í–‰

1. Hugging Face transformers ë¡œë“œ
    - mps(Apple Metal Performance Shader) ì‚¬ìš©
    - float16ìœ¼ë¡œ ë©”ëª¨ë¦¬ ìµœì í™”
    - LangChain + Vector DB ì ìš© (RAG)

2. ChromaDBë¡œ ë¬¸ì„œ ì €ì¥
    - LangChainì„ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰-ì‘ë‹µ ì‹œìŠ¤í…œ êµ¬ì¶•
    - Llama 3.1 8B Finetuning (LoRA)

3. PEFT(LoRA) í™œìš©í•˜ì—¬ ê°€ë²¼ìš´ í•™ìŠµ ì§„í–‰
    - Apple Silicon í™˜ê²½ì—ì„œë„ ê°€ëŠ¥

### í•™ìŠµëœ ëª¨ë¸ ì‚¬ìš© ë°©ë²•


```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

# Load the base model from its original checkpoint
base_model_name = "../models/Llama-3.1-8b"
base_model = AutoModelForCausalLM.from_pretrained(base_model_name)

# Now load the adapter from your local directory
model = PeftModel.from_pretrained(base_model, "finetuned-llama3", local_files_only=True)

# Load the tokenizer (you can load from your adapter folder if saved or from the base model)
tokenizer = AutoTokenizer.from_pretrained("finetuned-llama3")

prompt = "Question: Llama ëª¨ë¸ì€?\nAnswer:"
inputs = tokenizer(prompt, return_tensors="pt")

with torch.no_grad():
    outputs = model.generate(**inputs, max_new_tokens=50)

generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
print("Generated Response:")
print(generated_text)

```


    Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]


    Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.


    Generated Response:
    Question: Llama ëª¨ë¸ì€?
    Answer: B


Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
Generated Response:
Question: Llama ëª¨ë¸ì€?
Answer: B


  
ì˜ í•™ìŠµë˜ì§„ ì•Šì€ ëª¨ìŠµì´ë‹¤.
ì•„ë§ˆ ë„ˆë¬´ ì ì€ í¬ê¸°ì˜ ë°ì´í„°ì…‹ì„ ì´ìš©í•´ì„œ ê·¸ëŸ° ê²ƒ ê°™ë‹¤
í¬ê¸°ë¥¼ ëŠ˜ë ¤ì„œ ì‹œë„í•´ë³´ë©´ ì˜ ë  ê²ƒ ê°™ìŠµë‹ˆë‹¤

https://huggingface.co/datasets/SGTCho/korean_food
  
ì—ì„œ í•œì‹ ê´€ë ¨ ë°ì´í„°ì…‹ì„ ë°›ì€í›„

https://github.com/SGT-Cho/LLM/tree/main/Finetuning
  
ë”°ë¼í•´ë³´ë©´ ë  ê²ƒ ê°™ìŠµë‹ˆë‹¤


